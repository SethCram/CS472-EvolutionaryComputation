{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID  air_time1  disp_index1  gmrt_in_air1  gmrt_on_paper1  \\\n",
      "0      id_1       5160     0.000013    120.804174       86.853334   \n",
      "1      id_2      51980     0.000016    115.318238       83.448681   \n",
      "2      id_3       2600     0.000010    229.933997      172.761858   \n",
      "3      id_4       2130     0.000010    369.403342      183.193104   \n",
      "4      id_5       2310     0.000007    257.997131      111.275889   \n",
      "..      ...        ...          ...           ...             ...   \n",
      "169  id_170       2930     0.000010    241.736477      176.115957   \n",
      "170  id_171       2140     0.000009    274.728964      234.495802   \n",
      "171  id_172       3830     0.000008    151.536989      171.104693   \n",
      "172  id_173       1760     0.000008    289.518195      196.411138   \n",
      "173  id_174       2875     0.000008    235.769350      178.208024   \n",
      "\n",
      "     max_x_extension1  max_y_extension1  mean_acc_in_air1  mean_acc_on_paper1  \\\n",
      "0                 957              6601          0.361800            0.217459   \n",
      "1                1694              6998          0.272513            0.144880   \n",
      "2                2333              5802          0.387020            0.181342   \n",
      "3                1756              8159          0.556879            0.164502   \n",
      "4                 987              4732          0.266077            0.145104   \n",
      "..                ...               ...               ...                 ...   \n",
      "169              1839              6439          0.253347            0.174663   \n",
      "170              2053              8487          0.225537            0.174920   \n",
      "171              1287              7352          0.165480            0.161058   \n",
      "172              1674              6946          0.518937            0.202613   \n",
      "173              1838              6560          0.567311            0.147818   \n",
      "\n",
      "     mean_gmrt1  ...  mean_jerk_in_air25  mean_jerk_on_paper25  \\\n",
      "0    103.828754  ...            0.141434              0.024471   \n",
      "1     99.383459  ...            0.049663              0.018368   \n",
      "2    201.347928  ...            0.178194              0.017174   \n",
      "3    276.298223  ...            0.113905              0.019860   \n",
      "4    184.636510  ...            0.121782              0.020872   \n",
      "..          ...  ...                 ...                   ...   \n",
      "169  208.926217  ...            0.119152              0.020909   \n",
      "170  254.612383  ...            0.174495              0.017640   \n",
      "171  161.320841  ...            0.114472              0.017194   \n",
      "172  242.964666  ...            0.114472              0.017194   \n",
      "173  206.988687  ...            0.114472              0.017194   \n",
      "\n",
      "     mean_speed_in_air25  mean_speed_on_paper25  num_of_pendown25  \\\n",
      "0               5.596487               3.184589                71   \n",
      "1               1.665973               0.950249               129   \n",
      "2               4.000781               2.392521                74   \n",
      "3               4.206746               1.613522               123   \n",
      "4               3.319036               1.680629                92   \n",
      "..                   ...                    ...               ...   \n",
      "169             4.508709               2.233198                96   \n",
      "170             4.685573               2.806888                84   \n",
      "171             3.493815               2.510601                88   \n",
      "172             3.493815               2.510601                88   \n",
      "173             3.493815               2.510601                88   \n",
      "\n",
      "     paper_time25  pressure_mean25  pressure_var25  total_time25  class  \n",
      "0           40120      1749.278166     296102.7676        144605      P  \n",
      "1          126700      1504.768272     278744.2850        298640      P  \n",
      "2           45480      1431.443492     144411.7055         79025      P  \n",
      "3           67945      1465.843329     230184.7154        181220      P  \n",
      "4           37285      1841.702561     158290.0255         72575      P  \n",
      "..            ...              ...             ...           ...    ...  \n",
      "169         44545      1798.923336     247448.3108         80335      H  \n",
      "170         37560      1725.619941     160664.6464        345835      H  \n",
      "171         51675      1915.573488     128727.1241         83445      H  \n",
      "172         51675      1915.573488     128727.1241         83445      H  \n",
      "173         51675      1915.573488     128727.1241         83445      H  \n",
      "\n",
      "[174 rows x 452 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 51\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[39m#x_train = np.reshape(x_train, [x_train.shape[0], x_train.shape[1] * x_train.shape[2]]) #adds 2nd dim of 1\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[39m#x_test = np.reshape(x_test, [x_test.shape[0], x_test.shape[1] * x_test.shape[2]]) #adds 2nd dim of 1\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m x_train, y_train, x_test, y_test\n\u001b[1;32m---> 51\u001b[0m x_train, y_train, x_test, y_test \u001b[39m=\u001b[39m load_data()\n\u001b[0;32m     53\u001b[0m \u001b[39m#print('Data shape:', 'x_train:', x_train.shape, 'x_test:', x_test.shape)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m#print('Data shape:', 'y_train:', y_train.shape, 'y_test:', y_test.shape)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [20], line 43\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(show_sample)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nImg\u001b[39m*\u001b[39mnImg):\n\u001b[0;32m     42\u001b[0m         plt\u001b[39m.\u001b[39msubplot(nImg, nImg, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m         plt\u001b[39m.\u001b[39mimshow(x_train[i], cmap \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mGreys_r\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     46\u001b[0m \u001b[39m#x_train = np.reshape(x_train, [x_train.shape[0], x_train.shape[1] * x_train.shape[2]]) #adds 2nd dim of 1\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m#x_test = np.reshape(x_test, [x_test.shape[0], x_test.shape[1] * x_test.shape[2]]) #adds 2nd dim of 1\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load the alzheimers dataset\n",
    "def load_data(show_sample = True):\n",
    "    \n",
    "    ## (1) Data preparation\n",
    "    df=pd.read_csv('AlzData.csv', sep = ',')\n",
    "    print(df)\n",
    "    \n",
    "    #x_test = np.empty((2384, 988, 988))\n",
    "    \n",
    "    #for i in range(x_test.shape[0]):\n",
    "    #    x_test[i] = cv.imread(\"testImgs/\" + str(i) + \".png\", 0)\n",
    "    \n",
    "    #need to do some sort of zero/same padding to get same image sizes\n",
    "    # unless using fully convolutional NN bc can take inputs of different sizes\n",
    "    # or if use a spatial pyramid pooling (SPP) layer before dense layers\n",
    "    \n",
    "    #Could possibly create more training data thru upscaling/downscaling\n",
    "    # would better recognise diff scaled data\n",
    "    \n",
    "    # rm ids, rm targets\n",
    "    transposedDataFrameVals = df.values.T[1:-1]\n",
    "    #walk thru features\n",
    "    for i, row in enumerate(transposedDataFrameVals):\n",
    "        #normalize data [0,1]\n",
    "        transposedDataFrameVals[i] = row.astype(float) / max(row.astype(float))\n",
    "    \n",
    "    #assign normalized data\n",
    "    X = transposedDataFrameVals.T\n",
    "    #assign targets\n",
    "    Y = df.values.T[-1]\n",
    "        \n",
    "    #shapes of both are messed up\n",
    "    \n",
    "    #store whether benign (0?) or malignant (1?)\n",
    "    #y_train = \n",
    "    #y_test = \n",
    "        \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "#print('Data shape:', 'x_train:', x_train.shape, 'x_test:', x_test.shape)\n",
    "#print('Data shape:', 'y_train:', y_train.shape, 'y_test:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, None, None, 8)     80        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, None, 8)     0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, None, None, 8)    32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, None, None, 8)     0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, None, None, 16)    1168      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, None, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, None, None, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, None, None, 16)    0         \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,344\n",
      "Trainable params: 1,296\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# expand dims for channels?\n",
    "\n",
    "desiredLevels = 2\n",
    "\n",
    "def CreateConvLayer(convLayers, model, filters, kernel_size, activation = None, padding = \"valid\"):\n",
    "    conv = layers.Conv2D(\n",
    "        filters= filters, #num of filters for conv\n",
    "        kernel_size = kernel_size, \n",
    "        padding = padding,\n",
    "        activation = activation,\n",
    "        #input_shape = (28, 28, 1) #only 28 params \n",
    "    )\n",
    "    \n",
    "    convLayers.append(conv)\n",
    "    model.add(conv)\n",
    "    \n",
    "def CreateMaxPoolLayer(poolLayers, model, pool_size, strides):\n",
    "    pool = layers.MaxPooling2D(\n",
    "        pool_size = pool_size,\n",
    "        strides = strides,\n",
    "    )\n",
    "    \n",
    "    poolLayers.append(pool)\n",
    "    model.add(pool)\n",
    "    \n",
    "def CreateConvBlock(\n",
    "    model, convLayers, poolLayers, normLayers, activationLayers,\n",
    "    filters, kernel_size, activation,\n",
    "    pool_size, strides\n",
    "):\n",
    "    CreateConvLayer(convLayers, model, filters, kernel_size)\n",
    "    #CreateMaxPoolLayer(poolLayers, model, pool_size, strides)\n",
    "    \n",
    "    dropout = layers.Dropout(0.2)\n",
    "    model.add(dropout)\n",
    "    \n",
    "    norm = layers.BatchNormalization()\n",
    "    model.add(norm)\n",
    "    normLayers.append(norm)\n",
    "\n",
    "    activ = layers.Activation(activation)\n",
    "    model.add(activ)\n",
    "    activationLayers.append(activ)\n",
    "    \n",
    "# Create a fully conv NN\n",
    "model_fcn = keras.Sequential()\n",
    "\n",
    "convLayers = []\n",
    "poolLayers = []\n",
    "normLayers = []\n",
    "activationLayers = []\n",
    "\n",
    "# Input layer\n",
    "input = layers.Input(shape=(None, None, 1))\n",
    "model_fcn.add(input)\n",
    "\n",
    "CreateConvBlock(\n",
    "    model_fcn, convLayers, poolLayers, normLayers, activationLayers,\n",
    "    filters=8, kernel_size=3, activation='relu',\n",
    "    pool_size=2, strides=2\n",
    ")\n",
    "\n",
    "CreateConvBlock(\n",
    "    model_fcn, convLayers, poolLayers, normLayers, activationLayers,\n",
    "    filters=16, kernel_size=3, activation='softmax',\n",
    "    pool_size=2, strides=2\n",
    ")\n",
    "\n",
    "globMaxPool = layers.GlobalMaxPooling2D()\n",
    "model_fcn.add(globMaxPool)\n",
    "\n",
    "model_fcn(inputs=x_train, outputs=y_train)\n",
    "\n",
    "#need normalization and output layers\n",
    "\n",
    "#specify optimization\n",
    "model_fcn.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_fcn.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "history = model_fcn.fit(\n",
    "    x_train, #should be 2d inputs\n",
    "    y_train, #already onehot\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test accuracy\n",
    "y_test_pred = model_fcn.predict(x_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1) #still need to do bc pred is prob?\n",
    "test_err = np.sum(y_test == y_test_pred) / y_test.shape[0]\n",
    "\n",
    "print(f'test accuracy: {test_err * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history of training and validation accuracu\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training acc', 'validation acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get access to filters/kernels\n",
    "weights = convLayers[0].get_weights()[0][:, :, 0, :] #arr of a buncha nested lists, 0 bc only have 1 channel (28, 28, 1, 8)\n",
    "\n",
    "#can get access to other params this way \n",
    "\n",
    "for i in range(1, 8):\n",
    "    plt.subplot(2, 4, i)\n",
    "    plt.imshow(weights[:,:,i], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "#how get access to feature maps?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('EvoComp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe32189b82d98f8d4c6c56307210a678aeb8b7128cbceacf7b4eb9894a56bcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
