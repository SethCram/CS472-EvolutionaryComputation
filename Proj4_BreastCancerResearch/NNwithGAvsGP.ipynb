{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Seth Cram\n",
    "Class: Evolutionary Computation - CS472/CS572\n",
    "\n",
    "Project 4\n",
    "\n",
    "Due Date: ?\n",
    "\n",
    "Instructions:\n",
    "\n",
    "You should read the page and familiarize yourself with the way the data\n",
    "is formatted. Note that not all the data should be used when training your classifiers, you should select part of the data to be used to train your neural network/GP, part of the data should be reserved as a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Cells\n",
    "\n",
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID  air_time1  disp_index1  gmrt_in_air1  gmrt_on_paper1  \\\n",
      "0      id_1       5160     0.000013    120.804174       86.853334   \n",
      "1      id_2      51980     0.000016    115.318238       83.448681   \n",
      "2      id_3       2600     0.000010    229.933997      172.761858   \n",
      "3      id_4       2130     0.000010    369.403342      183.193104   \n",
      "4      id_5       2310     0.000007    257.997131      111.275889   \n",
      "..      ...        ...          ...           ...             ...   \n",
      "169  id_170       2930     0.000010    241.736477      176.115957   \n",
      "170  id_171       2140     0.000009    274.728964      234.495802   \n",
      "171  id_172       3830     0.000008    151.536989      171.104693   \n",
      "172  id_173       1760     0.000008    289.518195      196.411138   \n",
      "173  id_174       2875     0.000008    235.769350      178.208024   \n",
      "\n",
      "     max_x_extension1  max_y_extension1  mean_acc_in_air1  mean_acc_on_paper1  \\\n",
      "0                 957              6601          0.361800            0.217459   \n",
      "1                1694              6998          0.272513            0.144880   \n",
      "2                2333              5802          0.387020            0.181342   \n",
      "3                1756              8159          0.556879            0.164502   \n",
      "4                 987              4732          0.266077            0.145104   \n",
      "..                ...               ...               ...                 ...   \n",
      "169              1839              6439          0.253347            0.174663   \n",
      "170              2053              8487          0.225537            0.174920   \n",
      "171              1287              7352          0.165480            0.161058   \n",
      "172              1674              6946          0.518937            0.202613   \n",
      "173              1838              6560          0.567311            0.147818   \n",
      "\n",
      "     mean_gmrt1  ...  mean_jerk_in_air25  mean_jerk_on_paper25  \\\n",
      "0    103.828754  ...            0.141434              0.024471   \n",
      "1     99.383459  ...            0.049663              0.018368   \n",
      "2    201.347928  ...            0.178194              0.017174   \n",
      "3    276.298223  ...            0.113905              0.019860   \n",
      "4    184.636510  ...            0.121782              0.020872   \n",
      "..          ...  ...                 ...                   ...   \n",
      "169  208.926217  ...            0.119152              0.020909   \n",
      "170  254.612383  ...            0.174495              0.017640   \n",
      "171  161.320841  ...            0.114472              0.017194   \n",
      "172  242.964666  ...            0.114472              0.017194   \n",
      "173  206.988687  ...            0.114472              0.017194   \n",
      "\n",
      "     mean_speed_in_air25  mean_speed_on_paper25  num_of_pendown25  \\\n",
      "0               5.596487               3.184589                71   \n",
      "1               1.665973               0.950249               129   \n",
      "2               4.000781               2.392521                74   \n",
      "3               4.206746               1.613522               123   \n",
      "4               3.319036               1.680629                92   \n",
      "..                   ...                    ...               ...   \n",
      "169             4.508709               2.233198                96   \n",
      "170             4.685573               2.806888                84   \n",
      "171             3.493815               2.510601                88   \n",
      "172             3.493815               2.510601                88   \n",
      "173             3.493815               2.510601                88   \n",
      "\n",
      "     paper_time25  pressure_mean25  pressure_var25  total_time25  class  \n",
      "0           40120      1749.278166     296102.7676        144605      P  \n",
      "1          126700      1504.768272     278744.2850        298640      P  \n",
      "2           45480      1431.443492     144411.7055         79025      P  \n",
      "3           67945      1465.843329     230184.7154        181220      P  \n",
      "4           37285      1841.702561     158290.0255         72575      P  \n",
      "..            ...              ...             ...           ...    ...  \n",
      "169         44545      1798.923336     247448.3108         80335      H  \n",
      "170         37560      1725.619941     160664.6464        345835      H  \n",
      "171         51675      1915.573488     128727.1241         83445      H  \n",
      "172         51675      1915.573488     128727.1241         83445      H  \n",
      "173         51675      1915.573488     128727.1241         83445      H  \n",
      "\n",
      "[174 rows x 452 columns]\n",
      "Data shape: x_train: (121, 450) x_test: (53, 450)\n",
      "Data shape: y_train: (121,) y_test: (53,)\n"
     ]
    }
   ],
   "source": [
    "## load the alzheimers dataset\n",
    "def load_data(test_split = 0.3):\n",
    "    \n",
    "    ## (1) Data preparation\n",
    "    df=pd.read_csv('AlzData.csv', sep = ',')\n",
    "    print(df)\n",
    "    \n",
    "    # rm ids, rm targets\n",
    "    transposedDataFrameVals = df.values.T[1:-1].astype('float32')\n",
    "    #walk thru features\n",
    "    for i, row in enumerate(transposedDataFrameVals):\n",
    "        #normalize data [0,1]\n",
    "        transposedDataFrameVals[i] = row / max(row)\n",
    "    \n",
    "    #assign normalized data\n",
    "    X = transposedDataFrameVals.T\n",
    "    \n",
    "    #assign targets\n",
    "    targets = df.values.T[-1]\n",
    "    #conv to binary target\n",
    "    Y = np.array( [1 if target == 'P' else 0 for target in targets] )\n",
    "\n",
    "    # data split of 70 training and 30 test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_split)\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "print('Data shape:', 'x_train:', x_train.shape, 'x_test:', x_test.shape)\n",
    "print('Data shape:', 'y_train:', y_train.shape, 'y_test:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP \n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GP imports\n",
    "import copy\n",
    "from enum import Enum\n",
    "import random\n",
    "import time\n",
    "import scipy.stats as ss\n",
    "import anytree\n",
    "from functools import reduce\n",
    "import operator as OPER\n",
    "import sklearn.model_selection as sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IF(ops):\n",
    "    conditional, trueRslt, falseRslt = ops[0], ops[1], ops[2]\n",
    "    \n",
    "    #print(f\"if({conditional}) then {trueRslt} else {falseRslt}\")\n",
    "    \n",
    "    if conditional:\n",
    "        return trueRslt\n",
    "    else:\n",
    "        return falseRslt\n",
    " \n",
    "def SUBTRACT(ops):\n",
    "    #print(f\"{ops[0]} - {ops[1]}\")\n",
    "    \n",
    "    return reduce(OPER.sub, ops)\n",
    "\n",
    "def MULTIPLY(ops):\n",
    "    #print(f\"{ops[0]} * {ops[1]}\")\n",
    "    \n",
    "    return reduce(OPER.mul, ops)\n",
    "    \n",
    "def DIVIDE(ops):\n",
    "    \"\"\"Protected divison\n",
    "\n",
    "    Args:\n",
    "        ops (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(f\"{ops[0]} / {ops[1]}\")\n",
    "    \n",
    "    if( ops[1] == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return ops[0] / ops[1]\n",
    "    \n",
    "class Operator():\n",
    "    def __init__(self, funct, arity) -> None:\n",
    "        self.funct = funct\n",
    "        self.arity = arity\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.funct}, arity {self.arity}\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.funct}, arity {self.arity}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enum Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitType(Enum):\n",
    "    GROWTH = 0\n",
    "    FULL = 1\n",
    "    \n",
    "class NodeType(Enum):\n",
    "    TERMINAL = 0\n",
    "    NONTERMINAL = 1\n",
    "\n",
    "# objective function\n",
    "def objective(x):\n",
    "\treturn -(1.4 - 3.0 * x) * np.sin(18.0 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual():\n",
    "    def __init__(self, initDepth: int, initType: InitType, NT: set, T: set, x, y, softCapNodeMax) -> None:\n",
    "        #var init\n",
    "        self.initType = initType\n",
    "        self.initDepth = initDepth\n",
    "        self.T = T\n",
    "        self.NT = NT\n",
    "        self.softCapNodeMax = softCapNodeMax\n",
    "        \n",
    "        assert len(x) == len(y)\n",
    "        \n",
    "        #funct init\n",
    "        \n",
    "        #initialize individual as tree\n",
    "        self.nodeIndex = 0\n",
    "        self.root = self.CreateNodeNT(self.nodeIndex, parent=None)\n",
    "        self.CreateTreeRecursively(self.root)\n",
    "        #print(anytree.RenderTree(self.root))\n",
    "        \n",
    "        assert self.nodeIndex == self.GetNodeCount() - 1\n",
    "        #print(f\"self node count = {self.nodeCount}, get node count = {self.GetNodeCount()}\")\n",
    "        \n",
    "        #fitness eval of tree\n",
    "        self.EvaluateFitness(x, y)\n",
    "       \n",
    "    def EvaluateFitness(self, x, y, applyParsimonyPressure = True):\n",
    "        \"\"\"\n",
    "        Fitness evaluated through using RMSE (Root Mean Sqrd Error).\n",
    "        Applies parsimony pressure by default.\n",
    "        Lower fitness is better.\n",
    "        \"\"\"\n",
    "        \n",
    "        sqrdSum = 0\n",
    "        \n",
    "        inputCount = len(x)\n",
    "        \n",
    "        assert inputCount == len(y)\n",
    "        \n",
    "        for i in range(inputCount):\n",
    "            #NT's assigned values\n",
    "            self.EvaluateFitnessRecursively(self.root, x[i])\n",
    "            #accrue sqrd error\n",
    "            sqrdSum += ( y[i] - float( self.root.value ) )**2\n",
    "        \n",
    "        fitness = np.sqrt(sqrdSum/inputCount)\n",
    "            \n",
    "        nodeCount = self.GetNodeCount()\n",
    "            \n",
    "        #if applying pressure and enough nodes to apply fitness mod\n",
    "        if applyParsimonyPressure and nodeCount > self.softCapNodeMax:\n",
    "            #incr fitness by every additional node over the max\n",
    "            fitness = fitness * (nodeCount / self.softCapNodeMax)\n",
    "            \n",
    "        self.fitness = fitness\n",
    "       \n",
    "    def EvaluateFitnessRecursively(self, parent: anytree.node, x: float):\n",
    "        \"\"\"NT nodes assigned values.\n",
    "\n",
    "        Args:\n",
    "            parent (anytree.node): _description_\n",
    "        \"\"\"\n",
    "        ops = []\n",
    "        #walk thru children\n",
    "        for child in parent.children:\n",
    "            #if child is NT and not evaluated\n",
    "            if (child.type == NodeType.NONTERMINAL):\n",
    "                #evaluate child (don't change input val)\n",
    "                self.EvaluateFitnessRecursively(child, x)\n",
    "            #if child val is a variable \n",
    "            if( child.value == 'x'):\n",
    "                #substitute passed in var\n",
    "                ops.append(x)\n",
    "            #regular child \n",
    "            else:\n",
    "                ops.append(child.value)\n",
    "        \n",
    "        #evaluate parent using children values\n",
    "        parent.value = float( parent.operator.funct(ops) )\n",
    "       \n",
    "    def CreateTreeRecursively(self, parent: anytree.Node) -> None:\n",
    "            #every parent is a NT\n",
    "            for _ in range(parent.operator.arity):\n",
    "                self.nodeIndex += 1\n",
    "                nodeName = self.nodeIndex\n",
    "                \n",
    "                #if creating laster layer of nodes\n",
    "                if parent.depth == self.initDepth - 2: #depth starts at 0\n",
    "                        #create T node\n",
    "                        self.CreateNodeT(nodeName, parent) \n",
    "                #if not creating last layer\n",
    "                else:\n",
    "                    if self.initType == InitType.FULL:\n",
    "                        #recursively create NT\n",
    "                        self.CreateTreeRecursively( self.CreateNodeNT(nodeName, parent) )\n",
    "                    elif self.initType == InitType.GROWTH:\n",
    "                        #roll a 50/50 on whether child is T or NT\n",
    "                        if np.random.randint(0,2) == 0:\n",
    "                            #recursively create NT\n",
    "                            self.CreateTreeRecursively( self.CreateNodeNT(nodeName, parent) )\n",
    "                        else:\n",
    "                            self.CreateNodeT(nodeName, parent)\n",
    "              \n",
    "    def CreateNodeNT(self, nodeName, parent) -> anytree.Node:\n",
    "        #create a NT child node\n",
    "        return anytree.Node(nodeName, \n",
    "            operator=random.choice(tuple(self.NT)),\n",
    "            type = NodeType.NONTERMINAL,  \n",
    "            value = 0,                                   \n",
    "            parent = parent\n",
    "        )\n",
    "    \n",
    "    def CreateNodeT(self, nodeName, parent) -> anytree.Node:\n",
    "        #create a T child node\n",
    "        return anytree.Node(nodeName, \n",
    "            value = random.choice(tuple(self.T)), \n",
    "            type = NodeType.TERMINAL, \n",
    "            parent = parent\n",
    "        )\n",
    "       \n",
    "    def GetNodeCount(self) -> int:\n",
    "        \"\"\"Calcs node count through counting the root's descendants.\n",
    "        Needs to dynamically calculate node count bc during crossover, tree size changes.\n",
    "\n",
    "        Returns:\n",
    "            int: _description_\n",
    "        \"\"\"\n",
    "        #return number of descendants and 1 to account for root\n",
    "        return len(self.root.descendants) + 1\n",
    "       \n",
    "    #def Mutate(self):\n",
    "    #    num_of_nodes = len(self.tree)\n",
    "    \n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if not isinstance(other, Individual):\n",
    "            # don't attempt to compare against unrelated types\n",
    "            return NotImplemented\n",
    "        \n",
    "        return self.GetNodeCount() == other.GetNodeCount() #should iteratively compare each node\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{anytree.RenderTree(self.root)}, fitness of {self.fitness}\"  \n",
    "\n",
    "def getFitness( individual: Individual ) -> int:\n",
    "    return individual.fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Program Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP():\n",
    "    \"\"\"\n",
    "    Genetic Program with individuals as trees.\n",
    "    \"\"\"\n",
    "    def __init__(self, populationSize: int, initDepth: int, NT: set, T: set, x_train, y_train, pairs_of_parents_elitism_saves, softCapNodeMax: int = 10, xrate: float = 1):\n",
    "        self.populationSize = populationSize\n",
    "        self.initDepth = initDepth\n",
    "        self.NT = NT\n",
    "        self.T = T\n",
    "        self.xrate = xrate\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        #self.selectionType\n",
    "        self.currentGeneration = 0\n",
    "        self.pairs_of_parents_elitism_saves = pairs_of_parents_elitism_saves\n",
    "        \n",
    "        #create pop of 50/50 growth/full individuals\n",
    "        self.population = [Individual(initDepth, InitType.FULL, NT, T, x_train, y_train, softCapNodeMax) for _ in range(int(self.populationSize/2))] + [Individual(initDepth, InitType.GROWTH, NT, T, x_train, y_train, softCapNodeMax) for _ in range(int(self.populationSize/2))] \n",
    "        \n",
    "        #init fitness lists w/ starting pop's fitness vals\n",
    "        self.avgFitness = [self.GetAvgFitness()]\n",
    "        self.bestFitness = [self.GetBestFitness()]\n",
    "        self.worstFitness = [self.GetWorstFitness()] \n",
    "        self.bestFitnessNodeCount = [self.GetBestFitnessNodeCount()]\n",
    "        self.worstFitnessNodeCount = [self.GetWorstFitnessNodeCount()]\n",
    "           \n",
    "    def RunGen(self) -> None:\n",
    "        #create new pop\n",
    "        self.CreateNextGeneration()\n",
    "        \n",
    "        #store newly created pops fitness fields\n",
    "        self.avgFitness.append( self.GetAvgFitness() ) \n",
    "        self.worstFitness.append( self.GetWorstFitness() ) \n",
    "        self.bestFitness.append( self.GetBestFitness() )\n",
    "        self.bestFitnessNodeCount.append(self.GetBestFitnessNodeCount())\n",
    "        self.worstFitnessNodeCount.append(self.GetWorstFitnessNodeCount())\n",
    "        \n",
    "        #advance gen count\n",
    "        self.currentGeneration += 1\n",
    "   \n",
    "    def CreateNextGeneration(self) ->None:\n",
    "        #ensure individuals sorted in ascending order\n",
    "        self.OrderPopulationByFitness()\n",
    "        #new pop\n",
    "        newPopulation = []\n",
    "        \n",
    "        #Save parents for elitism \n",
    "        for k in range(0, self.pairs_of_parents_elitism_saves):\n",
    "            newPopulation.append(self.population[k])\n",
    "            newPopulation.append(self.population[k+1])\n",
    "        \n",
    "        pairs_of_children = int(self.populationSize/2)\n",
    "        \n",
    "        #walk thru half pop\n",
    "        for _ in range(self.pairs_of_parents_elitism_saves, pairs_of_children):\n",
    "            #select parents\n",
    "            parent1, parent2 = self.SelectParents()\n",
    "            #do crossover\n",
    "            child1, child2, xover = self.Crossover(parent1, parent2, self.xrate)\n",
    "            #if crossover happened\n",
    "            if(xover):\n",
    "                #re'eval children fitness\n",
    "                child1.EvaluateFitness(self.x_train, self.y_train)\n",
    "                child2.EvaluateFitness(self.x_train, self.y_train)\n",
    "            #add new children to next gen pop\n",
    "            newPopulation.append(child1)\n",
    "            newPopulation.append(child2)\n",
    "            \n",
    "        #don't needa deep copy bc newPopulation wiped out w/ leave funct\n",
    "        self.population = newPopulation\n",
    "   \n",
    "    def GetBestFitIndividual(self) -> Individual:\n",
    "        return min( self.population, key=getFitness )\n",
    "    \n",
    "    def GetWorstFitIndividual(self) -> Individual:\n",
    "        return max( self.population, key=getFitness )\n",
    "   \n",
    "    def GetBestFitnessNodeCount(self) -> int:\n",
    "        return self.GetBestFitIndividual().GetNodeCount()\n",
    "    \n",
    "    def GetWorstFitnessNodeCount(self) -> int:\n",
    "        return self.GetWorstFitIndividual().GetNodeCount()\n",
    "   \n",
    "    def GetBestFitness(self) -> float:\n",
    "        return self.GetBestFitIndividual().fitness\n",
    "    \n",
    "    def GetWorstFitness(self) -> float:\n",
    "        return self.GetWorstFitIndividual().fitness\n",
    "    \n",
    "    def GetAvgFitness(self) -> float:\n",
    "        fitnessSum = 0\n",
    "        for i in range(0, self.populationSize):\n",
    "            #take the fitness sum\n",
    "            fitnessSum +=  self.population[i].fitness\n",
    "        \n",
    "        return fitnessSum / self.populationSize\n",
    "    \n",
    "    def Crossover(self, parent1: Individual, parent2: Individual, xrate: float = 1) -> tuple:\n",
    "        \"\"\"Swaps subtree parents at their xpoints. \n",
    "        Xpoints gauss centered around last leaf.\n",
    "        Never chooses the root node to do crossover with.\n",
    "\n",
    "        Args:\n",
    "            parent1 (Individual): _description_\n",
    "            parent2 (Individual): _description_\n",
    "\n",
    "        Returns:\n",
    "            tuple: child1, child2, whether xover happened\n",
    "        \"\"\"\n",
    "        \n",
    "        #clone children from parents\n",
    "        child1 = copy.deepcopy(parent1) \n",
    "        child2 = copy.deepcopy(parent2)\n",
    "        \n",
    "        #roll on whether to do crossover\n",
    "        randProb = np.random.random()\n",
    "        xover = randProb <= xrate\n",
    "        if( xover ):\n",
    "        \n",
    "            #pick crossover subtress\n",
    "            parent1subtree, parent2subtree = self.GetCrossoverSubtrees(child1, child2)\n",
    "        \n",
    "            \"\"\"\n",
    "            #if rand is 80% of time\n",
    "            if( np.random.randint(1,11) <= 8):\n",
    "                \n",
    "                #while they're both terminals\n",
    "                while( \n",
    "                    parent1subtree.type  == NodeType.TERMINAL and \n",
    "                    parent2subtree.type  == NodeType.TERMINAL \n",
    "                ):\n",
    "                    #pick new crossover point for both\n",
    "                    parent1subtree, parent2subtree = self.GetCrossoverSubtrees(child1, child2)\n",
    "                \n",
    "                #now atleast one is a NT\n",
    "            \n",
    "            #if rand is 20% of time\n",
    "            else:\n",
    "                #while they're both NTs\n",
    "                while( \n",
    "                    parent1subtree.type == NodeType.NONTERMINAL and \n",
    "                    parent2subtree.type == NodeType.NONTERMINAL \n",
    "                ):\n",
    "                    #pick new crossover point for both\n",
    "                    parent1subtree, parent2subtree = self.GetCrossoverSubtrees(child1, child2)\n",
    "                    \n",
    "                #now atleast one is a T\n",
    "            \"\"\"\n",
    "            \n",
    "            #swap subtree parents (don't copy)\n",
    "            parent1subtree_parent_ph = parent1subtree.parent \n",
    "            #print(anytree.RenderTree(child1.root))\n",
    "            #print(anytree.RenderTree(child2.root))\n",
    "            parent1subtree.parent = parent2subtree.parent\n",
    "            parent2subtree.parent = parent1subtree_parent_ph\n",
    "            #print(anytree.RenderTree(child1.root))\n",
    "            #print(anytree.RenderTree(child2.root))\n",
    "\n",
    "        return child1, child2, xover\n",
    "    \n",
    "    def GetCrossoverSubtrees(self, parent1, parent2) -> tuple:\n",
    "        \"\"\"Swaps subtrees at last leaf gauss random indices.\n",
    "\n",
    "        Args:\n",
    "            parent1 (_type_): _description_\n",
    "            parent2 (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            tuple: child1, child2 still connected to parent1 and parent2 (not copies)\n",
    "        \"\"\"\n",
    "        \n",
    "        #use whatever parent has less nodes to choose xpoint\n",
    "        #if( parent1.GetNodeCount() < parent2.GetNodeCount()):\n",
    "        #    xpointUpperBound = parent1.GetNodeCount()\n",
    "        #else:\n",
    "        #    xpointUpperBound = parent2.GetNodeCount()\n",
    "        #pick crossover points\n",
    "        #p1_xpoint, p2_xpoint = np.random.randint(1, xpointUpperBound, size=2)\n",
    "        #parent1subtree = anytree.find(parent1.root, filter_= lambda node: node.name == p1_xpoint) #could also index root descendants instead\n",
    "        #parent2subtree = anytree.find(parent2.root, filter_= lambda node: node.name == p2_xpoint)\n",
    "        \n",
    "        #cache parent node counts\n",
    "        p1Nodes = parent1.GetNodeCount()\n",
    "        p2Nodes = parent2.GetNodeCount()\n",
    "        #find descendant node count\n",
    "        p1descendantNodes = p1Nodes - 1\n",
    "        p2descendantNodes = p2Nodes - 1\n",
    "        \n",
    "        #gen half-normal range of ints centered at 0\n",
    "        # std dev of 1/4 of descendant nodes count\n",
    "        p1xIndexRange, p1prob = self.SetupHalfNormIntDistr(p1descendantNodes, stdDev=p1descendantNodes/4)\n",
    "        p2xIndexRange, p2prob = self.SetupHalfNormIntDistr(p2descendantNodes, stdDev=p2descendantNodes/4)\n",
    "        \n",
    "        #p1_xpoint = int( np.random.randint(0, p1Nodes-1, size=1) )\n",
    "        #p2_xpoint = int ( np.random.randint(0, p2Nodes-1, size=1) )\n",
    "        #parent1subtree = parent1.root.descendants[p1_xpoint]\n",
    "        #parent2subtree = parent2.root.descendants[p2_xpoint]\n",
    "        \n",
    "        #sel parent xpoints from 1 to descendant nodes count\n",
    "        p1_xpoint = int( np.random.choice(p1xIndexRange+1, size = 1, p = p1prob) )\n",
    "        p2_xpoint = int( np.random.choice(p2xIndexRange+1, size = 1, p = p2prob) )\n",
    "        \n",
    "        #apply xpoint, starting from the end\n",
    "        # so norm distr centered around end of list (more terminals, smaller NTs)\n",
    "        parent1subtree = parent1.root.descendants[-p1_xpoint]\n",
    "        parent2subtree = parent2.root.descendants[-p2_xpoint]\n",
    "        \n",
    "        #debug: print(f\"Crossover at {parent1subtree.name} and {parent2subtree.name}\")\n",
    "        \n",
    "        assert parent1subtree != None, f\"Couldn't find a node with xpoint {-p1_xpoint-1} in tree {anytree.RenderTree(parent1.root)}\"\n",
    "        assert parent2subtree != None, f\"Couldn't find a node with xpoint {-p2_xpoint-1} in tree {anytree.RenderTree(parent2.root)}\"\n",
    "        \n",
    "        return parent1subtree, parent2subtree\n",
    "    \n",
    "    def SelectParents(self) -> tuple:\n",
    "        xIndexRange, prob = self.SetupHalfNormIntDistr(self.populationSize, stdDev=30)\n",
    "    \n",
    "        #if overloaded to display distr graph\n",
    "        if(False):\n",
    "            #take randos using the calc'd prob and index range\n",
    "            nums = np.random.choice(xIndexRange, size = 1000000, p = prob)\n",
    "            #display distr histogram\n",
    "            plt.rcParams.update({'font.size': 22})\n",
    "            plt.hist(nums, bins = pop_size)\n",
    "            plt.title(\"likelihood of each parent index being chosen\")\n",
    "            plt.ylabel(\"likelihood of being chosen\")\n",
    "            plt.xlabel(\"parent index\")\n",
    "            plt.show()\n",
    "\n",
    "        #get parent indices\n",
    "        parent1Index, parent2Index = np.random.choice(xIndexRange, size = 2, p = prob)\n",
    "        #parent1Index, parent2Index = parentIndices[0], parentIndices[1]\n",
    "        \n",
    "        #make sure indices within array range\n",
    "        #assert parent1Index < self.populationSize and parent2Index < self.populationSize and type(parent1Index) == int and type(parent2Index) == int\n",
    "    \n",
    "        return self.population[int(parent1Index)], self.population[int(parent2Index)]\n",
    "    \n",
    "    def SetupHalfNormIntDistr(self, pop_size: int, stdDev: int) -> tuple:\n",
    "        \"\"\"\n",
    "        The half normal integer distribution parent indices are drawn from.\n",
    "\n",
    "        Returns:\n",
    "            tuple: index range and probability funct\n",
    "        \"\"\"\n",
    "        #take interval 1-100\n",
    "        x = np.arange(1, pop_size+1) #bc upper bound is exclusive\n",
    "        #store every number's +/-0.5\n",
    "        xU, xL = x + 0.5, x - 0.5 \n",
    "        #determine probability\n",
    "        prob = ss.halfnorm.cdf(xU, scale = stdDev) - ss.halfnorm.cdf(xL, scale = stdDev) #scale represents inner quartiles\n",
    "        prob = prob / prob.sum() # normalize the probabilities so their sum is 1\n",
    "        #decr by 1 to find the index 0-99\n",
    "        xIndexRange = x - 1\n",
    "    \n",
    "        return xIndexRange, prob\n",
    "    \n",
    "    def OrderPopulationByFitness(self):\n",
    "        #sort in descending order\n",
    "        self.population.sort(key=getFitness)\n",
    "    \n",
    "    def PlotGenerationalFitness(self):\n",
    "        t = np.arange(0, self.currentGeneration+1)\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "        plt.title('Generational Fitness Data')\n",
    "        plt.plot(t, self.worstFitness, label='Worst Fitness') \n",
    "        plt.plot(t, self.avgFitness, label='Average Fitness') \n",
    "        plt.plot(t, self.bestFitness, label='Best Fitness') \n",
    "        plt.grid() \n",
    "        plt.legend()\n",
    "        plt.ylabel('Fitness')\n",
    "        plt.xlabel('Generation')\n",
    "        \n",
    "        #init worst worst fit\n",
    "        worstWorstFitness = max(self.worstFitness)\n",
    "        \n",
    "        fitnessIndex = 0\n",
    "        \n",
    "        for fitnessData in (self.worstFitness, self.avgFitness, self.bestFitness):\n",
    "            yAnnotatePosition = worstWorstFitness - worstWorstFitness * fitnessIndex / 12\n",
    "            \n",
    "            fitnessIndex += 1\n",
    "            \n",
    "            plt.annotate('%0.7f' % min(fitnessData), xy=(1, yAnnotatePosition), xytext=(8, 0), \n",
    "                        xycoords=('axes fraction', 'data'), textcoords='offset points')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def PlotGenerationalNodeCount(self):\n",
    "        t = np.arange(0, self.currentGeneration+1)\n",
    "            \n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "        plt.plot(t, self.bestFitnessNodeCount, label='Best Fitness') \n",
    "        plt.plot(t, self.worstFitnessNodeCount, label='Worst Fitness')\n",
    "        plt.grid() \n",
    "        plt.legend()\n",
    "        plt.title('Node Count per Generation')\n",
    "        plt.ylabel('Node Count')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.show()\n",
    "    \n",
    "    def Predict(self, x) -> list:\n",
    "        \"\"\"\n",
    "        Predict using the best fit individual.\n",
    "        \"\"\"\n",
    "        inputCount = len(x)\n",
    "        \n",
    "        y_pred = np.empty(inputCount)\n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred = np.empty((self.populationSize, inputCount))\n",
    "        \n",
    "        for j in range(self.populationSize):\n",
    "            for i in range(inputCount):\n",
    "                self.population[j].EvaluateFitnessRecursively(self.root, x[i])\n",
    "                \n",
    "                y_pred[j][i] = self.population[j].root.value\n",
    "        \"\"\"\n",
    "        \n",
    "        #sel best individual\n",
    "        bestIndividual = self.GetBestFitIndividual()\n",
    "        \n",
    "        #print(\"Best Individual:\")\n",
    "        #print(anytree.RenderTree(bestIndividual.root))\n",
    "        \n",
    "        for i in range(inputCount):\n",
    "            bestIndividual.EvaluateFitnessRecursively(bestIndividual.root, x[i])\n",
    "            \n",
    "            y_pred[i] = bestIndividual.root.value\n",
    "            \n",
    "        return y_pred\n",
    "        \n",
    "    def Test(self, x, y, training_bounds: tuple):\n",
    "        \n",
    "        inputCount = len(x)\n",
    "        \n",
    "        assert inputCount == len(y)\n",
    "        \n",
    "        #for i in range(self.populationSize): #why re-eval fitness for new data??\n",
    "        #    self.population[i].EvaluateFitness(x, y)\n",
    "            \n",
    "        bestFitIndividual = self.GetBestFitIndividual()\n",
    "        \n",
    "        print(\"Best fit individual:\")\n",
    "        print(anytree.RenderTree(bestFitIndividual.root))\n",
    "        \n",
    "        y_pred = self.Predict(x)\n",
    "        \n",
    "        assert len(y_pred) == len(y)\n",
    "        \n",
    "        #t = np.arange(0, inputCount)\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "        plt.plot(x, y_pred, label='Predictions') \n",
    "        plt.plot(x, y, label='Targets') \n",
    "        plt.legend()\n",
    "        plt.grid() \n",
    "        plt.title('Predictions vs Targets')\n",
    "        plt.ylabel('y')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylim(-20, 20)\n",
    "        # draw a vertical line at the optimal input\n",
    "        plt.axvline(x=training_bounds[0], ls='--', color='red')\n",
    "        plt.axvline(x=training_bounds[1], ls='--', color='red')\n",
    "        plt.text(training_bounds[0] + 0.1, 5,'training range') #(training_bounds[1] - training_bounds[0])/2\n",
    "        plt.show()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.population}, best fitness {self.bestFitness[-1]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Implementation\n",
    "### Implementation Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = {\n",
    "        Operator(funct=sum, arity=2), \n",
    "        Operator(funct=SUBTRACT, arity=2), \n",
    "        Operator(funct=MULTIPLY, arity=2), \n",
    "        Operator(funct=DIVIDE, arity=2), #division by zero always yields zero in integer arithmetic.\n",
    "        Operator(funct=np.abs, arity=1),\n",
    "        Operator(funct=IF, arity=3),\n",
    "        Operator(funct=np.sin, arity=1),\n",
    "    }\n",
    "\n",
    "# define optimal input value\n",
    "#x_optima = 0.96609\n",
    "#construct terminal set\n",
    "T = {0, 1, 1.4, 3, 18, np.pi, 'x'}\n",
    "\n",
    "POPULATION_SIZE = 100\n",
    "INDIVIDUALS_NUMBER_OF_TRAITS = 10\n",
    "GENERATIONS_PER_RUN = 300  \n",
    "PAIRS_OF_PARENTS_SAVED_FOR_ELITISM = 1\n",
    "XRATE = 0.95\n",
    "INIT_DEPTH = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m): \u001b[39m#used to be 20\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m     \u001b[39m#test GP\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     gp \u001b[39m=\u001b[39m GP(\n\u001b[0;32m     20\u001b[0m         populationSize\u001b[39m=\u001b[39;49mPOPULATION_SIZE,\n\u001b[0;32m     21\u001b[0m         initDepth\u001b[39m=\u001b[39;49mINIT_DEPTH,\n\u001b[0;32m     22\u001b[0m         NT\u001b[39m=\u001b[39;49mNT,\n\u001b[0;32m     23\u001b[0m         T\u001b[39m=\u001b[39;49mT,\n\u001b[0;32m     24\u001b[0m         xrate\u001b[39m=\u001b[39;49mXRATE,\n\u001b[0;32m     25\u001b[0m         x_train\u001b[39m=\u001b[39;49mx_train,\n\u001b[0;32m     26\u001b[0m         y_train\u001b[39m=\u001b[39;49my_train,\n\u001b[0;32m     27\u001b[0m         pairs_of_parents_elitism_saves\u001b[39m=\u001b[39;49mPAIRS_OF_PARENTS_SAVED_FOR_ELITISM\n\u001b[0;32m     28\u001b[0m     )\n\u001b[0;32m     30\u001b[0m     \u001b[39m#validation data # sample input range uniformly at 0.01 increments\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[39m#x_validation = np.arange(0.9, r_max, 0.001)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[39m#y_validation = objective(x_validation)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(GENERATIONS_PER_RUN):\n",
      "Cell \u001b[1;32mIn [25], line 18\u001b[0m, in \u001b[0;36mGP.__init__\u001b[1;34m(self, populationSize, initDepth, NT, T, x_train, y_train, pairs_of_parents_elitism_saves, softCapNodeMax, xrate)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpairs_of_parents_elitism_saves \u001b[39m=\u001b[39m pairs_of_parents_elitism_saves\n\u001b[0;32m     17\u001b[0m \u001b[39m#create pop of 50/50 growth/full individuals\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation \u001b[39m=\u001b[39m [Individual(initDepth, InitType\u001b[39m.\u001b[39mFULL, NT, T, x_train, y_train, softCapNodeMax) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulationSize\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))] \u001b[39m+\u001b[39m [Individual(initDepth, InitType\u001b[39m.\u001b[39mGROWTH, NT, T, x_train, y_train, softCapNodeMax) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulationSize\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))] \n\u001b[0;32m     20\u001b[0m \u001b[39m#init fitness lists w/ starting pop's fitness vals\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgFitness \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGetAvgFitness()]\n",
      "Cell \u001b[1;32mIn [25], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpairs_of_parents_elitism_saves \u001b[39m=\u001b[39m pairs_of_parents_elitism_saves\n\u001b[0;32m     17\u001b[0m \u001b[39m#create pop of 50/50 growth/full individuals\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation \u001b[39m=\u001b[39m [Individual(initDepth, InitType\u001b[39m.\u001b[39;49mFULL, NT, T, x_train, y_train, softCapNodeMax) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulationSize\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))] \u001b[39m+\u001b[39m [Individual(initDepth, InitType\u001b[39m.\u001b[39mGROWTH, NT, T, x_train, y_train, softCapNodeMax) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulationSize\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))] \n\u001b[0;32m     20\u001b[0m \u001b[39m#init fitness lists w/ starting pop's fitness vals\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgFitness \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGetAvgFitness()]\n",
      "Cell \u001b[1;32mIn [24], line 24\u001b[0m, in \u001b[0;36mIndividual.__init__\u001b[1;34m(self, initDepth, initType, NT, T, x, y, softCapNodeMax)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodeIndex \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGetNodeCount() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[39m#print(f\"self node count = {self.nodeCount}, get node count = {self.GetNodeCount()}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[39m#fitness eval of tree\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEvaluateFitness(x, y)\n",
      "Cell \u001b[1;32mIn [24], line 41\u001b[0m, in \u001b[0;36mIndividual.EvaluateFitness\u001b[1;34m(self, x, y, applyParsimonyPressure)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39massert\u001b[39;00m inputCount \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(y)\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(inputCount):\n\u001b[0;32m     40\u001b[0m     \u001b[39m#NT's assigned values\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEvaluateFitnessRecursively(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, x[i])\n\u001b[0;32m     42\u001b[0m     \u001b[39m#accrue sqrd error\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sqrdSum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ( y[i] \u001b[39m-\u001b[39m \u001b[39mfloat\u001b[39m( \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot\u001b[39m.\u001b[39mvalue ) )\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n",
      "Cell \u001b[1;32mIn [24], line 68\u001b[0m, in \u001b[0;36mIndividual.EvaluateFitnessRecursively\u001b[1;34m(self, parent, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m parent\u001b[39m.\u001b[39mchildren:\n\u001b[0;32m     65\u001b[0m     \u001b[39m#if child is NT and not evaluated\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m (child\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m NodeType\u001b[39m.\u001b[39mNONTERMINAL):\n\u001b[0;32m     67\u001b[0m         \u001b[39m#evaluate child (don't change input val)\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEvaluateFitnessRecursively(child, x)\n\u001b[0;32m     69\u001b[0m     \u001b[39m#if child val is a variable \u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m( child\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     71\u001b[0m         \u001b[39m#substitute passed in var\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [24], line 68\u001b[0m, in \u001b[0;36mIndividual.EvaluateFitnessRecursively\u001b[1;34m(self, parent, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m parent\u001b[39m.\u001b[39mchildren:\n\u001b[0;32m     65\u001b[0m     \u001b[39m#if child is NT and not evaluated\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m (child\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m NodeType\u001b[39m.\u001b[39mNONTERMINAL):\n\u001b[0;32m     67\u001b[0m         \u001b[39m#evaluate child (don't change input val)\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEvaluateFitnessRecursively(child, x)\n\u001b[0;32m     69\u001b[0m     \u001b[39m#if child val is a variable \u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m( child\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     71\u001b[0m         \u001b[39m#substitute passed in var\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [24], line 78\u001b[0m, in \u001b[0;36mIndividual.EvaluateFitnessRecursively\u001b[1;34m(self, parent, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m         ops\u001b[39m.\u001b[39mappend(child\u001b[39m.\u001b[39mvalue)\n\u001b[0;32m     77\u001b[0m \u001b[39m#evaluate parent using children values\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m parent\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39;49m( parent\u001b[39m.\u001b[39;49moperator\u001b[39m.\u001b[39;49mfunct(ops) )\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# define range for input\n",
    "strt, stp = 0.0, 1.2\n",
    "# define optimal input value\n",
    "x_optima = 0.96609\n",
    "# sample input range uniformly at 0.01 increments\n",
    "inputs = np.arange(strt, stp, 0.01)\n",
    "# compute targets\n",
    "results = objective(inputs)\n",
    "\n",
    "#test individual class\n",
    "#individual1 = Individual(INIT_DEPTH, InitType.FULL, NT, T, inputs, results)\n",
    "#individual2 = Individual(INIT_DEPTH, InitType.GROWTH, NT, T, inputs, results)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(2): #used to be 20\n",
    "\n",
    "    #test GP\n",
    "    gp = GP(\n",
    "        populationSize=POPULATION_SIZE,\n",
    "        initDepth=INIT_DEPTH,\n",
    "        NT=NT,\n",
    "        T=T,\n",
    "        xrate=XRATE,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        pairs_of_parents_elitism_saves=PAIRS_OF_PARENTS_SAVED_FOR_ELITISM\n",
    "    )\n",
    "    \n",
    "    #validation data # sample input range uniformly at 0.01 increments\n",
    "    #x_validation = np.arange(0.9, r_max, 0.001)\n",
    "    #y_validation = objective(x_validation)\n",
    "    \n",
    "    for _ in range(GENERATIONS_PER_RUN):\n",
    "    \n",
    "        gp.RunGen()\n",
    "\n",
    "    #if first GP\n",
    "    if( i == 0 ):\n",
    "        #make it the best\n",
    "        bestFitnessGP = copy.copy( gp.bestFitness[-1] )\n",
    "        bestGP = gp\n",
    "    #if not first GP\n",
    "    else:\n",
    "        #if curr gp performs better than prev best\n",
    "        if(gp.bestFitness[-1] < bestFitnessGP):\n",
    "            #establish new best fitness GP\n",
    "            bestFitnessGP = copy.copy( gp.bestFitness[-1] )\n",
    "            bestGP = gp\n",
    "    \n",
    "print(f\"Runtime took {time.time() - start_time} seconds.\")\n",
    "    \n",
    "bestGP.PlotGenerationalFitness()\n",
    "bestGP.PlotGenerationalNodeCount()\n",
    "\n",
    "#test using data outside of input range\n",
    "# define range for input\n",
    "r_min, r_max = -1, 5\n",
    "# sample input range uniformly at increments\n",
    "x_test = np.arange(r_min, r_max, 0.001)\n",
    "# compute targets\n",
    "y_test = objective(x_test)\n",
    "#compare to test set outside of bounds\n",
    "bestGP.Test(x_test, y_test, training_bounds = (strt, stp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN imports \n",
    "import keras \n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "from keras import losses\n",
    "from keras import applications\n",
    "#import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                28864     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,977\n",
      "Trainable params: 30,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a Fully Connected NN\n",
    "\n",
    "features = x_train.shape[1]\n",
    "hidden_nodes = (64, 32)\n",
    "\n",
    "model_fcn = keras.Sequential()\n",
    "\n",
    "hiddenLayers = []\n",
    "\n",
    "h_layer = layers.Dense(\n",
    "    input_dim=features,\n",
    "    units=hidden_nodes[0],\n",
    "    activation=activations.relu,\n",
    ")\n",
    "hiddenLayers.append(h_layer)\n",
    "model_fcn.add(h_layer)\n",
    "\n",
    "h_layer = layers.Dense(\n",
    "    input_dim=features,\n",
    "    units=hidden_nodes[1],\n",
    "    activation=activations.relu,\n",
    ")\n",
    "hiddenLayers.append(h_layer)\n",
    "model_fcn.add(h_layer)\n",
    "\n",
    "o_layer = layers.Dense(\n",
    "    input_dim=hidden_nodes[0],\n",
    "    units=1,\n",
    "    activation=activations.softmax,\n",
    ")\n",
    "model_fcn.add(o_layer)\n",
    "\n",
    "#need normalization and output layers\n",
    "\n",
    "#specify optimization\n",
    "model_fcn.compile(optimizer=optimizers.Adam(), loss=losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "model_fcn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.5312 - val_loss: 0.5592 - val_accuracy: 0.4800\n",
      "Epoch 2/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.5312 - val_loss: 0.5561 - val_accuracy: 0.4800\n",
      "Epoch 3/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.5312 - val_loss: 0.5655 - val_accuracy: 0.4800\n",
      "Epoch 4/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.5312 - val_loss: 0.5811 - val_accuracy: 0.4800\n",
      "Epoch 5/50\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 0.5312 - val_loss: 0.5805 - val_accuracy: 0.4800\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.5312 - val_loss: 0.6334 - val_accuracy: 0.4800\n",
      "Epoch 7/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.5312 - val_loss: 0.5942 - val_accuracy: 0.4800\n",
      "Epoch 8/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.5312 - val_loss: 0.6123 - val_accuracy: 0.4800\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.5312 - val_loss: 0.6377 - val_accuracy: 0.4800\n",
      "Epoch 10/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.5312 - val_loss: 0.6173 - val_accuracy: 0.4800\n",
      "Epoch 11/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.5312 - val_loss: 0.6208 - val_accuracy: 0.4800\n",
      "Epoch 12/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.5312 - val_loss: 0.6527 - val_accuracy: 0.4800\n",
      "Epoch 13/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.5312 - val_loss: 0.6326 - val_accuracy: 0.4800\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.5312 - val_loss: 0.6319 - val_accuracy: 0.4800\n",
      "Epoch 15/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.5312 - val_loss: 0.6460 - val_accuracy: 0.4800\n",
      "Epoch 16/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.5312 - val_loss: 0.6547 - val_accuracy: 0.4800\n",
      "Epoch 17/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.5312 - val_loss: 0.6665 - val_accuracy: 0.4800\n",
      "Epoch 18/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.5312 - val_loss: 0.6853 - val_accuracy: 0.4800\n",
      "Epoch 19/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.5312 - val_loss: 0.6793 - val_accuracy: 0.4800\n",
      "Epoch 20/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.5312 - val_loss: 0.6842 - val_accuracy: 0.4800\n",
      "Epoch 21/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.5312 - val_loss: 0.6902 - val_accuracy: 0.4800\n",
      "Epoch 22/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.5312 - val_loss: 0.7052 - val_accuracy: 0.4800\n",
      "Epoch 23/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.5312 - val_loss: 0.7057 - val_accuracy: 0.4800\n",
      "Epoch 24/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.5312 - val_loss: 0.7048 - val_accuracy: 0.4800\n",
      "Epoch 25/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.5312 - val_loss: 0.7076 - val_accuracy: 0.4800\n",
      "Epoch 26/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.5312 - val_loss: 0.7202 - val_accuracy: 0.4800\n",
      "Epoch 27/50\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.5312 - val_loss: 0.7525 - val_accuracy: 0.4800\n",
      "Epoch 28/50\n",
      " 1/48 [..............................] - ETA: 0s - loss: 0.0071 - accuracy: 0.5000"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "history = model_fcn.fit(\n",
    "    x_train, \n",
    "    y_train, #already onehot\n",
    "    batch_size=2,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "test accuracy: 49.056603773584904%\n"
     ]
    }
   ],
   "source": [
    "#test accuracy\n",
    "y_test_pred = model_fcn.predict(x_test)[:, 0] #cutoff last dim bc useless\n",
    "truePreds = np.sum(y_test == y_test_pred)\n",
    "test_acc = truePreds / y_test.shape[0]\n",
    "\n",
    "print(f'test accuracy: {test_acc * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAGsCAYAAAD9mPKEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZOElEQVR4nO3deVxU9f4/8NcwbILDLiCCYKgkauaumKGiomS5lHua2pdst1y62iJaXpfUrku3q+nNLUNNU1wxr+KG4sUFKzXNBcMQEIEB2cTh8/uDH+cOMuthx9fz8ZjH4zPM+3zO58DAvDjL5yiEEAJEREREZBaLmh4AERERUV3EEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDJY1vQA6rPi4mIkJydDpVJBoVDU9HCIiIjIBEII5OTkwMvLCxYW+vc3MURVoeTkZPj4+NT0MIiIiEiGpKQkeHt7632dIaoKqVQqACU/BAcHhxoeDREREZkiOzsbPj4+0ue4PgxRVaj0EJ6DgwNDFBERUR1j7FQcnlhOREREJANDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDJU+N55u3fvxqZNmxAfH4+UlBQ4ODigefPmGDp0KCZPnlxp94zr1asXjh07ZnL9rVu34Ofnp/O1v/76CydOnMDZs2dx7tw5JCcnIz09HdnZ2bC3t0fTpk3RpUsXjB49GiEhIZUy/soihEB+kaamh0FERFQrNLBSGr3HXVVRCCGEnAUfPHiAsWPHYvfu3XprfHx8sG3bNnTr1k32AEtVZoh699138c9//tOkfkJCQhAZGYlGjRqZvO5S2dnZcHR0hFqtrrQwmffwEQJnH6yUvoiIiOq6y5+Hws66wvuEyjD181vWWjUaDYYPH47o6GgAgIeHB8LDwxEYGIiMjAxERkYiNjYWSUlJCAsLQ2xsLFq1aiVvS3TYuXOn0Rp3d3eDr1tbW6Njx47o2LEjWrRogUaNGsHKygr37t1DXFwcfvzxR+Tn5+Pw4cPo1asXzp49iwYNGlTWJhAREVEdJ2tP1OrVq/Hmm28CAAIDA3HkyBF4eHiUqZk+fTqWLl0KAOjZsyeOHz9eoYFq74mSufNMcvPmTXh4eMDe3l5vTWJiIvr27YsbN24AAL788kvMmDHDrPVUxZ4oHs4jIiL6n6o4nGfq57fZIUqj0cDHxwd3794FAJw7dw4dOnTQWdepUyckJCQAAA4ePIj+/fubs6oyKjNEmeqnn37Cyy+/DEBeEKyKEEVERERVy9TPb7Ovzjt+/LgUoIKDg3UGKABQKpV4//33peeRkZHmrqrGtW7dWmqnpKTU4EiIiIiotjE7RB04cEBqh4WFGawdOHCgzuXqiuvXr0ttT0/PGhwJERER1TZmh6hff/1Vanfu3NlgraenJ3x8fAAAqampuHfvnrmr02nQoEFo0qQJrK2t4ezsjNatWyM8PBwxMTGV0j8ApKWlYebMmdLzV155pdL6JiIiorrP7Kvzrl69KrWbNWtmtL5Zs2ZISkqSlpUzVcDj9u3bJ7WzsrKQlZWFy5cvY+3atejTpw++//57NG7c2KS+EhMTpfO2iouLkZGRgbNnz2LLli1Qq9UASvaovf322xUeNxEREdUfZoeorKwsqe3m5ma03tXVVeeycjg7O6Nfv37o1KkTmjRpAqVSib/++guHDx/GgQMHIITAkSNH0L17d8TFxZl0CC46OhpvvfWWzteaNm2Kt956Cx999BEsLDi5OxEREf2P2SHqwYMHUtvW1tZovfbcSjk5OeauTrJgwQJ07NgR1tbW5V6bOnUqzp49i5dffhl//vknbt++jUmTJmH//v2y16dUKhESEoIePXqYHKAKCwtRWFgoPc/Ozpa9fiIiIqrd6szule7du+sMUKU6deqE6Oho2NjYACg5kT0+Pt5ov2+++SaEEBBC4OHDh/jzzz+xefNmtG/fHuvWrcPzzz+PadOmQaMxPjfTggUL4OjoKD1KzwcjIiKi+sfsENWwYUOpXVBQYLQ+Pz9faqtUKnNXZ5ZWrVph3Lhx0vO9e/eatbyVlRV8fHwwZswYxMXF4dVXXwUAfPXVV/jss8+MLj9r1iyo1WrpUXouGBEREdU/ZocoJycnqZ2enm60/v79+zqXrSq9e/eW2leuXJHdj1KpxDfffANHR0cAwD/+8Q+j53TZ2NjAwcGhzIOIiIjqJ7NDVEBAgNS+deuW0XrtGu1lq4r21X8VPZFdpVLhueeeA1Cy1y0uLq5C/REREVH9YXaIatu2rdQ2ds5RamqqdEjL3d29UqY3MEZ771hl7PnSPgSZmZlZ4f6IiIiofjA7RA0YMEBqG5uFXPvqOGOzm1cW7Qk3K2PP1x9//CG1qyMEEhERUd1gdogKDg6W5l86evQozp8/r7NOo9FgxYoV0vNRo0bJHKLprl27hk2bNknPBw0aVKH+4uPjpe2ztrY2OkM7ERERPTnMDlFKpRKzZ8+Wno8fPx5paWnl6mbOnCnNBN6jRw+Ehobq7G/9+vVQKBRQKBTo1auXzpoVK1bg1KlTBsd14cIFhIaGSlcM9u/fH127di1Xd/HiRSxfvrzMfFe6/Pe//8XQoUMhhAAAjB07VjrJnIiIiMjsyTYBIDw8HDt37sShQ4dw6dIltGvXDuHh4QgMDERGRgYiIyNx8uRJACXnJa1evbpCgzxy5AimTJkCf39/9O3bF23atIGrqyuUSiWSk5Nx+PBh7N+/H8XFxQAAX19frFu3TmdfmZmZ+OCDDzBr1iyEhISgU6dO8PX1hUqlQn5+PhITExETE4OYmBgpQLVu3RpLliyp0DYQERFR/SIrRFlaWmLHjh0YM2YM9u7di5SUFHzxxRfl6ry9vbF161a0bt26wgMFgBs3buDGjRsGa0JDQ/Hdd9/By8vLYF1+fj727t1rdC6p0aNHY+XKlXBxcTF7vERERFR/yQpRQMlVa3v27EFUVBQ2btyI+Ph4pKWlQaVSwd/fH8OGDcPkyZMr5RDY0qVL8eKLL+LMmTO4ePEi0tLSkJ6ejsLCQjg6OsLPzw/du3fH2LFjdR7C0/b8888jLi4Ohw8fRnx8PH7//Xf89ddfyMvLg42NDZycnPD0008jKCgIo0ePRmBgYIXHT0RERPWPQpQes6JKl52dDUdHR6jVak68SUREVEeY+vldZ+6dR0RERFSbMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJANDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJANDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJANDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJEOFQ9Tu3bsxfPhw+Pn5wdbWFu7u7ggKCsLixYuRnZ1dGWMEAPTq1QsKhcLkR2Jiot6+hBCIi4vDvHnz8MILL8DPzw8NGjSAra0tvLy8MGDAACxfvhxZWVmVNn4iIiKqXxRCCCFnwQcPHmDs2LHYvXu33hofHx9s27YN3bp1kz3AUr169cKxY8dMrr916xb8/PzKff3atWsICQnBnTt3jPbh6uqK1atX4+WXXzZnqJLs7Gw4OjpCrVbDwcFBVh9ERERUvUz9/LaU07lGo8Hw4cMRHR0NAPDw8EB4eDgCAwORkZGByMhIxMbGIikpCWFhYYiNjUWrVq3kbYkOO3fuNFrj7u6u8+sZGRlSgLKxsUHv3r3Ro0cPNG3aFDY2Nrh+/To2b96MK1eu4P79+xgxYgQiIyMxYsSIShs/ERER1QNChlWrVgkAAoAIDAwUKSkp5WqmTZsm1fTs2VPOasoIDg6W+quI06dPCx8fH7FixQqRkZGhs6aoqEi888470vpcXFxEZmam2etSq9UCgFCr1RUaMxEREVUfUz+/zT6cp9Fo4OPjg7t37wIAzp07hw4dOuis69SpExISEgAABw8eRP/+/eVmvTKH88wcchm5ubmwsrKCtbW1wTohBDp16oTz588DANatW4cJEyaYtS4eziMiIqp7TP38NvvE8uPHj0sBKjg4WGeAAgClUon3339feh4ZGWnuqqqEvb290QAFAAqFAsOHD5ee//LLL1U5LCIiIqpjzA5RBw4ckNphYWEGawcOHKhzubpCO33m5+fX4EiIiIiotjE7RP36669Su3PnzgZrPT094ePjAwBITU3FvXv3zF2dToMGDUKTJk1gbW0NZ2dntG7dGuHh4YiJiamU/ktpb6uvr2+l9k1ERER1m9kh6urVq1K7WbNmRuu1a7SXrYh9+/YhOTkZRUVFyMrKwuXLl7F27Vr06dMHISEh0uHGisjMzMTWrVul5y+88EKF+yQiIqL6w+wpDrQnoHRzczNa7+rqqnNZOZydndGvXz906tQJTZo0gVKpxF9//YXDhw/jwIEDEELgyJEj6N69O+Li4uDp6Sl7XdOmTUNmZiYA4KWXXkLbtm2NLlNYWIjCwkLpeWVONkpERES1i9kh6sGDB1Lb1tbWaH2DBg2kdk5OjrmrkyxYsAAdO3bUeVL41KlTcfbsWbz88sv4888/cfv2bUyaNAn79++Xta5Vq1Zh3bp1AAAnJycsX77c5DHOnTtX1jqJiIiobqkz987r3r27wavqOnXqhOjoaNjY2AAoOZE9Pj7e7PXs27cP7733HgDAwsIC69at0znzuS6zZs2CWq2WHklJSWavn4iIiOoGs0NUw4YNpXZBQYHReu2r2lQqlbmrM0urVq0wbtw46fnevXvNWv4///kPXnnlFTx69AgKhQLffvsthgwZYvLyNjY2cHBwKPMgIiKi+snsEOXk5CS109PTjdbfv39f57JVpXfv3lL7ypUrJi935MgRvPTSSygoKIBCocC//vUvvP7661UxRCIiIqoHzA5RAQEBUvvWrVtG67VrtJetKo0aNZLapp7IfuTIEbz44ovSXrN//vOfmDx5clUMj4iIiOoJs0OU9lVqxs45Sk1Nlc4Lcnd3LxNwqor23jFT9nyVBqi8vDwAwMqVK/HWW29V1fCIiIionjA7RA0YMEBqG5uFXPvqOGOzm1cW7Qk3je35ejxALV++HO+++26Vjo+IiIjqB7NDVHBwsDT/0tGjR6Ub9D5Oo9FgxYoV0vNRo0bJHKLprl27hk2bNknPBw0apLf26NGjZQLUsmXLytzrj4iIiMgQs0OUUqnE7Nmzpefjx49HWlpaubqZM2ciISEBANCjRw+Ehobq7G/9+vVQKBRQKBTo1auXzpoVK1bg1KlTBsd14cIFhIaGSlcM9u/fH127dtVZe+zYMbzwwgtlAtSUKVMM9k9ERESkzezJNgEgPDwcO3fuxKFDh3Dp0iW0a9cO4eHhCAwMREZGBiIjI3Hy5EkAJeclrV69ukKDPHLkCKZMmQJ/f3/07dsXbdq0gaurK5RKJZKTk3H48GHs378fxcXFAEruc1c6WebjEhISygSo0NBQ+Pr6YteuXQbH4Obmhueee65C20FERET1h6wQZWlpiR07dmDMmDHYu3cvUlJS8MUXX5Sr8/b2xtatW9G6desKDxQAbty4gRs3bhisCQ0NxXfffQcvLy+dryckJCA3N1d6fvDgQRw8eNDouoODg3H06FGzxktERET1l6wQBZRMnLlnzx5ERUVh48aNiI+PR1paGlQqFfz9/TFs2DBMnjwZjo6OFR7k0qVL8eKLL+LMmTO4ePEi0tLSkJ6ejsLCQjg6OsLPzw/du3fH2LFj9R7CIyIiIqpMCiGEqOlB1FfZ2dlwdHSEWq3m7OVERER1hKmf33Xm3nlEREREtQlDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJANDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJANDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERyWBZ0wMgoqonhEBRURGKi4treihERJXOwsICVlZWUCgU1bpehiiieiwvLw9qtRo5OTnQaDQ1PRwioiqjVCqhUqng6OgIOzu7alknQxRRPZWTk4M7d+7AysoKTk5OsLe3h4WFRbX/p0ZEVJWEECguLkZubi6ys7ORlZUFb29vqFSqKl83QxRRPZSXl4c7d+7AwcEBXl5eDE5EVO/Z29ujUaNGSE5Oxp07d+Dr61vle6R4YjlRPaRWq2FlZcUARURPFIVCAS8vL1hZWUGtVlf5+hiiiOoZIQRycnLg4ODAAEVETxyFQgEHBwfk5ORACFGl62KIIqpnioqKoNFoYG9vX9NDISKqEXZ2dtBoNCgqKqrS9TBEEdUzpdMYWFjw15uInkxKpRIAqnxaF/6VJaqneCiPiJ5U1fX3jyGKiIiISAaGKCIiIiIZKhyidu/ejeHDh8PPzw+2trZwd3dHUFAQFi9ejOzs7MoYIwCgV69eUCgUJj8SExMN9nf37l3s2bMHc+bMwaBBg9C4ceMyyxMREREZInuyzQcPHmDs2LHYvXt3ma/fu3cP9+7dw+nTp7Fy5Ups27YN3bp1q/BAK9OePXvw0ksv1fQwiIiIqA6TFaI0Gg2GDx+O6OhoAICHhwfCw8MRGBiIjIwMREZGIjY2FklJSQgLC0NsbCxatWpVaYPeuXOn0Rp3d3eD49dmZWWFNm3a4MKFCxUeGxFRXderVy8cO3YMAKp8nh0A8PPzw+3bt+Hr62v0KAJRbSIrRK1du1YKUIGBgThy5Ag8PDyk19955x1Mnz4dS5cuRWZmJiZPnozjx49XzogBDBkypELLu7u7Izw8HB07dkTHjh3xzDPPwNramofxiKhKZGVlYdmyZQCAZ599tsJ/w4iodjA7RGk0GsydO1d6vmnTpjIBqtSiRYtw+PBhJCQk4MSJE/j555/Rv3//io22kgQFBSEoKKimh0FET4isrCzp7+Zrr73GEEVUT5h9Yvnx48dx9+5dAEBwcDA6dOigs06pVOL999+XnkdGRsocIhERVaejR49CCFEth/IAIDExEUIIHsqjOsfsEHXgwAGpHRYWZrB24MCBOpcjIiIiquvMDlG//vqr1O7cubPBWk9PT/j4+AAAUlNTce/ePXNXp9OgQYPQpEkTWFtbw9nZGa1bt0Z4eDhiYmIqpX8iIiIiY8wOUVevXpXazZo1M1qvXaO9bEXs27cPycnJKCoqQlZWFi5fvoy1a9eiT58+CAkJkQ43EhHVpMTERCgUijJ/Bzds2KBzbrujR49KNUePHpW+PmfOHADAH3/8gWnTpqF169ZwcnIq81qpO3fu4JtvvsGoUaMQGBgIlUoFKysruLm5oWvXrpg1axaSkpKMjlt7Xj5d1q9fL72+fv16AMC1a9fw3nvvoWXLlrCzs4OTkxO6d++O5cuX4+HDhwbX5+fnB4VCAT8/P52vz5kzp9z36dy5c5g4cSKeeuop2NrawtXVFb1798b69etNvl/ayZMnMXr0aHh7e8PW1hZNmjRBWFgYduzYAeB/Pz+FQoEJEyaY1Kc+jx49wqFDh/DRRx8hODgYjRs3hrW1Nezt7eHn54fhw4dj+/btZt3rLScnB8uWLcPAgQOlbWjQoAGeeuopDB06FP/617+QkZFhsI/ExER8+umnCAoKgoeHB6ytraFSqdCmTRtMmDAB27dvN/rze6IJMzk7OwsAAoDIyckxWj906FCpfs+ePeauThIcHCycnZ3FiBEjxJdffik2b94stmzZIpYuXSrCwsKEQqGQ1uPr6yvu3r1r9jpKl5fxbRFCCFFQUCDUarX0SEpKEgCEWq2W1R+RHPn5+eLy5csiPz+/pofyxLt161aZvyuGHjExMdJyMTEx0tcjIiLEpk2bRIMGDcotExERUWYZ7b+D+h7W1tZi7dq1BscdHBxs8G/hunXrpNfXrVsnNm7cqHN8pY/u3bsb/Dvo6+sr/e3WJSIiosz3aeHChUKpVOpd35AhQ0RRUZHBbZwxY4bB79eoUaPEH3/8IT1/7bXXDPZnTO/evU16HwQFBYmUlBSj/UVGRgoXFxej/Q0ZMkTn8o8ePRIzZ84UVlZWRvtYtmxZhba9JlT076BarTbp89vsq/MePHggtW1tbY3WN2jQQGrn5OSYuzrJggUL0LFjR1hbW5d7berUqTh79ixefvll/Pnnn7h9+zYmTZqE/fv3y16f3DFqX7lIRE82d3d37Ny5E2lpaZg8eTIAoHfv3mUuuinVpk0bnX2cOnUKf//736FQKPDaa6+hZ8+esLe3x/Xr19G0aVOprqCgAEIIBAQEoHfv3ggMDISbmxssLS2RkpKC48ePY9euXXj48CHCw8Ph4eGBQYMGVXgbo6OjsX37dtjZ2eGdd95B586dYWNjg4SEBKxatQpqtRqnT5/G9OnT8e2331Z4fWvWrMEPP/yARo0aYcKECXjmmWdgYWGBU6dOYe3atSgsLMSuXbvw5Zdf4uOPP9bZx7x587B48WIAJTeqHTZsGAYMGICGDRvi2rVr+O6777Blyxaz9goZk5eXB3t7e/Tq1QsdO3ZEs2bNoFKpkJubiytXruDHH3/EjRs3cOrUKQwdOhTHjx+HpaXuj+ivv/4a7733nvS8ffv2GDp0KPz9/WFhYYE7d+7g1KlTOHjwoM6LA4QQGD16NH788UfpezBw4ED069cPXl5eKCwsxPXr13H06FGcPHmy2i4wqJPMTWfaqdVY0hdCiDFjxkj1P/zwg7mrM8vly5eFjY2NtL7//ve/Zi0PreQtB/dEUW3APVG1j/YeKVP2aGjviQIg3N3dxcWLFw0uk5iYKBISEgzWXLhwQbi7uwsAokWLFqK4uFhnnTl7ogCI1q1bizt37pSru3LlimjYsKEAIKysrPTuYTFnTxQAERwcLLKyssrVHT16VNpD5ebmJgoLC8vVXL16Vfocs7KyElFRUeVqcnNzRb9+/cqss6J7og4dOiRyc3P1vl5UVCTeeecdaX2bNm3SWXfmzBlhaWkpAAhLS0vx7bff6u1TrVaLQ4cOlfv60qVLpfV4eHiI06dP6+3j5s2b4ty5cwa2rHaqrj1RZp8T1bBhQ6ldUFBgtD4/P19qq1Qqc1dnllatWmHcuHHS871791bp+h5nY2MDBweHMg+i2kwIgbyHj57oh6gD/2WvXr0azzzzjMEaX19ftGvXzmDNs88+i/nz5wMoOcfq1KlTFR6bpaUlfvrpJzRp0qTca08//TTeeecdAEBRURH+85//VHh9Li4u2LFjBxwdHcu9FhwcjFdeeQUAkJ6ejvj4+HI1X3/9NYqKigAA06dP13kLMDs7O/zwww9wcnKq8HhL9e3bF3Z2dnpft7S0xLJly6TzwjZs2KCzLiIiAo8ePQIAzJ8/H+Hh4Xr7dHBwQN++fct8LTc3V3oPKJVK7Nq1y+Ct2Zo1a6Z3KiOSMdmmk5MTMjMzAZS8SbVDlS73798vs2xV6927N9auXQsAuHLlSpWvj6guyy/SIHD2wZoeRo26/Hko7Kxl30a0yvn6+mLw4MGV1t9zzz0ntePi4tCjR48K9Tdo0CC0bNlS7+v9+vXDokWLAAC//fZbhdYFAOPHj4erq6vB9W3dulVa3+Pbt2vXLgCAhYWFzsOqpdzc3DBu3DisXLmywmM2laWlJbp164bExET897//hRCizMn99+7dw8GDJb+vnp6e+OCDD8xex4EDB6TP5cGDB9e6e9vWNWbviQoICJDat27dMlqvXaO9bFVp1KiR1M7Kyqry9RERVaUePXqYdUuqhIQETJ8+HT169IC7uztsbW3LXAX49NNPS7V37typ8Pi6d+9u8HVvb2+pXfoPeE2tLzU1Vbo6sVWrVvD09DTYV+/evWWOUre8vDysW7cOw4YNQ4sWLeDg4AALC4syP58tW7YAALKzs5GdnV1mee3zkwYOHAgrKyuzx3DixAmpXZnh/Ell9r9fbdu2le6bFx8fb/BNpv2GdXd3LxNwqkp6errUro49X0R1WQMrJS5/HlrTw6hRDayUNT0Eg7RDgSGPHj3CO++8gzVr1ph8iPLxD2k53NzcDL5uY2MjtU05BaQq15ecnCy1/f39ja7rqaeeMnN0+p06dQqjRo0yaYqJUtnZ2WUOW2qH3sDAQFnjqIw+6H/MDlEDBgyQrmo4cOAAPvroI7212lfHGZvdvLJoT7hZHXu+iOoyhUJRqw9lUdkrnA2ZMmWKdPWblZUVBgwYgC5dusDb2xv29vbSXgvtKwU1Gk2Fx2dhYfYBjRpbX25urtQ2dH5SKXt7e9nr0nbr1i2EhoZKV7c3b94cAwYMQMuWLeHm5ibtLQSAFStWSJ9jj/98tEOvsVNp9KmMPuh/zP7rGRwcDE9PT6SkpODo0aM4f/68zpPONBoNVqxYIT0fNWpUxUZqgmvXrmHTpk3S88q4fJeIqLZLSkrCqlWrAABNmjRBTEwMWrRoobP20qVL1Tm0WkU7FOXl5Rmt1w5dFTF//nwpQP3tb3/DggUL9B6i3bx5s95+tC9W0p5uyByV0Qf9j9mRXqlUYvbs2dLz8ePHIy0trVzdzJkzkZCQAKDkmH5oqO5DBtoz3/bq1UtnzYoVK4xeRXLhwgWEhoZKu2/79++Prl27mrBFRER123/+8x9pTqOZM2fqDVCAaeey1ldeXl5S+8aNG0brb968WSnr/fnnnwGUnNZSOueXPoZ+PtqHdi9fvixrLJXRB/2PrP344eHh2LlzJw4dOoRLly6hXbt2CA8PR2BgIDIyMhAZGYmTJ08CKDkvafXq1RUa5JEjRzBlyhT4+/ujb9++aNOmDVxdXaFUKpGcnIzDhw9j//790h8RX19frFu3zmCfS5cuNXiS46efflrmubOzM6ZNm1ah7SCiJ5P2IaiqmFIhJSVFajdv3txg7ZN8M3gPDw/4+PggKSkJV65cQUpKisGTyyvrfqylP59mzZpBqdR/Dt7du3dx8eJFva/37NkTCoUCQggcOHAARUVFZp9c/vzzz0tHiaKiojB+/HizlqeyZIUoS0tL7NixA2PGjMHevXuRkpKCL774olydt7c3tm7ditatW1d4oEDJfw7G/nsIDQ3Fd999V+Y/Dl1WrlyJ27dv633973//e5nnvr6+DFFEJIv2uSeVdYhIm/ZhquvXr+utu3nzpt75h54UgwcPxtdff43i4mKsWLFCmjPpcenp6WVOD6kIe3t7PHz4EDdu3Cg3bYG2zz//XJoDShc3NzcMHDgQ+/fvR0pKCpYtW4YZM2aYNZaBAwfCzc0N6enpiIqKQlxcHKc5qADZZ+ipVCrs2bMHu3btwrBhw+Dj4wMbGxvpRpeLFi3Cb7/9hqCgoAoPcunSpVi7di3Cw8PRpUsX+Pn5oWHDhtKNNTt16oT33nsPcXFxiI6ONhqgiIiqk4uLi3SVVUJCQqXvjercubPUXrJkSZn5+Ur9+eefePHFF6skxNUl7777rrT3ZsmSJdi9e3e5mry8PIwZM6bSpskp/fmkp6dj6dKlOmuWLl0qnddmyJw5c6TbwXz88cdYs2aN3tqcnBwcPny4zNfs7OzwySefACg5d3nIkCGIi4vT28ft27dx4cIFo+N6UlX4spzBgwdXaK6JCRMmGL07tr+/P/z9/fH666/LXs/jEhMTK60vIiJjQkJC8NNPP+HGjRsYMWIEhg0bBicnJ2mvRJcuXeDi4iKr7+7du6Nr1644c+YMbt++jaeffhpvvPEGWrVqBY1Gg7i4OGzatAm5ubmYMGEC1q9fX4lbVrcEBARg9uzZ+Oyzz1BUVIQhQ4ZI985TqVS4evUq1q1bh8TERIwYMQLbtm0DULGrAj/44APpvKgZM2YgJiYGAwYMgIeHB/78809s27YN8fHxaNy4Mdq2bSvV6tK5c2f84x//wHvvvYdHjx7hjTfewKpVqzB06FA89dRTsLCwQHJyMuLi4nDgwAGEhIQgJCSkTB9TpkxBbGwstm/fjtTUVAQFBSEsLAz9+vVD48aN8fDhQ9y8eRPHjh3DsWPHsGTJErRv31729tdnvLaZiKgaREREIDo6Gnl5edi+fTu2b99e5vWYmBi9F9eYYsuWLejTpw9u3bqF9PR0nYep3nvvPXz44YdPdIgCSs55VavVWLp0KYQQ2LFjB3bs2FGmZtSoUYiIiJBCVEVuWzZw4EDMmTMHc+bMAVAy/Y/2FEBAySkjP/30U5mr2vV599134eTkhHfffRdqtRrnz5/H+fPnddbqCn+lk3p+9NFHWL58OTQaDfbt24d9+/aZ3AeV4HeGiKgaPPPMM7hw4QImT56MVq1awd7e3qyZyI3x8/PDhQsXMGfOHDzzzDOws7ODnZ0dnnrqKbz66quIiYnBihUrKnWdddnixYtx7NgxjBgxAl5eXrC2tkbjxo0xYMAAbN++HZGRkVCr1VK93L2EpSIiInDkyBEMGTIEHh4esLKyQqNGjdC1a1csXLgQCQkJZt2j7tVXX8WtW7ewaNEi9OrVS+qzQYMG8Pf3x7Bhw/Dtt9/qDcxKpRJLly7F5cuXMWPGDHTo0AEuLi5QKpVQqVRo06YNJk2ahKioKLz99tsV2vb6TCHqwt0366jS2WbVajVvRkzVpqCgALdu3UKzZs1ga2tb08MhqrNWrlwp3V9v586dGDJkSM0OiExW0b+Dpn5+c08UERHRY4qKiqTpeaysrCp8o2aqnxiiiIjoiZKWlmZwosmCggJMmjRJmt39lVdeqZZ7v1LdwxPLiYjoifLnn3+ic+fO6NSpE0JCQhAQEAAHBwfk5OTgl19+wZYtW3D37l0AgKurK5YsWVLDI6baiiGKiIieSGfPnsXZs2f1vt6sWTNERUVx7kHSiyGKiIieKG3btkVkZCSio6Nx8eJF3Lt3T5qg1M3NDe3bt8eLL76I1157DdbW1jU8WqrNGKKIiOiJYmNjg1GjRmHUqFE1PRSq43hiOREREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJANDFBEREZEMDFFEREREMjBEERE9gdavXw+FQgGFQoH169frrPHz84NCoYCfn1+F1zdnzhxpfUePHq1wf5XJlO8FkS6WNT0AIiKiypSQkIBdu3YBAIYMGYJnn322RsdD9RdDFBER1SsJCQmYO3cugJK9aQxRVFUYooiISKfExMSaHkK1mDBhAiZMmFDTw6A6iOdEEREREcnAEEVEREQkA0MUEVEV0Wg08PLygkKhgEqlQl5entFlHj58CDc3NygUCri6uuLhw4dlXi8oKEBUVBTef/99BAUFoVGjRrCysoJKpUKLFi0wbtw4HDp0qFLGb+rVeRqNBqtXr0bPnj3h4uICOzs7tGjRAm+//TauXLli8voqum2lV9lNnDhR+trEiROlK++0H7qWM+XqvKysLCxcuBA9e/aEh4cHrK2t4e7ujueeew4LFixAVlaWweV1XaV47tw5TJw4EU899RRsbW3h6uqK3r17Y/369SguLjbYn6lycnKwZcsWTJ48GZ07d4aLiwusrKzg5OSEwMBAhIeH47///a9ZfSYmJuLTTz9FUFCQ9L1QqVRo06YNJkyYgO3bt5d7/z4uNjYWb7/9Ntq2bSuNycXFBV27dsWHH36IkydPVmSzq56gKqNWqwUAoVara3oo9ATJz88Xly9fFvn5+TU9FBJCTJ8+XQAQAMSmTZuM1u/cuVOqf+utt8q93qxZM+l1Q4/BgweLnJwcvetZt26dVLtu3TqdNb6+vgKA8PX11dtPenq66Ny5s95x2Nrais2bN4uIiAjpazExMTr7qui2aW+TsYe53wshhNi3b59wcXEx2K+Li4vYt2+f3j4e/z4sXLhQKJVKvf0NGTJEFBUV6e3PFIWFhcLW1tak78vkyZONru/Ro0di5syZwsrKymh/y5Yt09nH/fv3xaBBg0waU0JCgtnbXNG/g6Z+fvPEciKiKvTaa69hyZIlAIBNmzbh1VdfNVi/ceNGqT1+/Phyr+fl5cHJyQl9+vRB+/bt4evrCzs7O2RnZ+OXX37B1q1bcffuXURFRWHSpEnYtm1b5W6QlqKiIgwYMABnz54FALi4uOD111/Hs88+i8LCQhw9ehSbN2/GxIkT0a9fP6P9VXTb+vTpg507d+LIkSNYuXIlAOC9995Dnz59KrytBw8exODBg/Ho0SMAQNeuXTFq1Ch4eXnh7t272LJlC+Li4pCRkYHBgwdj7969CA0NNdjnmjVr8MMPP6BRo0aYMGECnnnmGVhYWODUqVNYu3YtCgsLsWvXLnz55Zf4+OOPZY+9uLgYBQUF8PDwQEhICNq1awcvLy80aNAAmZmZOHv2LLZt24bMzEysXr0aDg4O+PLLL3X2JYTA6NGj8eOPPwIAFAoFBg4ciH79+sHLywuFhYW4fv06jh49ipMnT0IIUa6PjIwMdO/eHdeuXQMA2NnZYcSIEejevTucnZ2Rk5OD3377DdHR0bhy5YrOPmoNWRGNTMI9UVQTuCeq9mnfvr0AIJRKpUhOTtZbd//+fWFtbS0AiJYtW+qs2b9/v3j48KHePnJzc8WQIUOk/+JPnDihs64y9kTNnz9f6uPpp5/WuW0nTpwQ9vb2ZfYs6NsTVZ3bZk59Tk6O8PDwkGrmzJkjiouLy9QUFxeL2bNnSzUeHh4iOzu7XF/ae6IAiODgYJGVlVWu7ujRo9IeKjc3N1FYWGh0O/R59OiR2L9/v9BoNHpr0tPTRffu3aX3aWJios66pUuXltnG06dP6+3z5s2b4ty5c+W+/uKLL0p9dOvWzeDvRGxsrLh7966BrdOtuvZE8ZwooieZEMDD3Cf7UQ3/5ZbuUdJoNPj+++/11m3btk06h2TcuHE6awYOHAgrKyu9fdjZ2WHDhg2wt7cHAGzYsEHusA0qKirC8uXLAQBKpRJbt25F48aNy9U999xzWLRokUl91pZte9y6deuQmpoKAAgLC0NERES586oUCgXmzp2LAQMGAABSU1Px3XffGezXxcUFO3bsgKOjY7nXgoOD8corrwAA0tPTER8fL3v8SqUSAwcOhIWF/o98V1dXaS+ovvdpbm4u5s+fL/W5a9cudOvWTW+fzZo1Q4cOHcp87cyZM9izZw8AwNvbG/v379f5vikVFBQET09P/RtXw3g4j+hJVpQHzPeq6VHUrI+TAWv7Kl3FmDFjMGPGDDx69AibNm3CjBkzdNaVfogpFAq9IcoUDg4OaNu2LeLi4hAXFye7H0NiY2OlYNG3b18888wzemv/7//+D59++qnRk65NUR3b9riffvpJav/tb38zWPvxxx8jOjpaWm7KlCl6a8ePHw9XV1e9r/fr1w9bt24FAPz222/o0aOHOcM2W/PmzeHp6YmUlBSd39sDBw7g/v37AIDBgwcbDFD6bNq0SWp/9NFHcHZ2lj/gWoB7ooiIqpi7u7u0h+LXX39FQkJCuZrr16/j9OnTAIDnn38evr6+evvLzMzE119/jUGDBsHPzw8qlQoWFhZlrj4r/RC8c+dO5W8QUOZKrpCQEIO1NjY2eO6550zqtzZsmzYhhLStdnZ2RrejR48e0p6y+Ph4g1fXde/e3WBf3t7eUjszM9PUIeuVnJyMRYsWoX///vD29oa9vX25qxZTUlIA6P7enjhxQmoPHjxY1hgqo4/ahHuiiJ5kVnYle2KeZFZ21bKa8ePHY+/evQBK9jg9fisS7cMnuk4oLxUVFYXXX39d2iNgTHZ2tvmDNUFy8v/eN82bNzdab0pNbdm2x9dROjWFv7+/wUNiAGBhYYHmzZvj4sWLyM/PR1ZWFlxcXHTWurm5GezLxsZGahcUFJg58rJWr16NqVOnmjTNBqD7e6sdrAIDA2WNo7QPe3t7NG3aVFYftQlDFNGTTKGo8kNZVOKll16Cs7MzMjMzERkZicWLF0OpVEqvlx7msLOzw/Dhw3X2cfr0abzyyivSFWLPPPMM+vbti+bNm8PZ2Rk2NjbSuTqffvopLl26VGnzDD3uwYMHUtvOzngQLd07o09t2jZtOTk5UtvYNpRq2LBhmeX1hShjgayy/Pjjj3jzzTel5927d0dwcDCaNWsGR0fHMmHtjTfewL1796DRaMr1ox2stLfRHKV9yF2+tmGIIiKqBjY2NhgxYgRWr16NlJQU/Pzzzxg4cCCAkvOLbt68CQAYMmQIVCqVzj5mz54thYx//vOfePvtt/Wu7+9//3slb0FZ2h+CpuzdyM3NNfh6bdo2bdo/C2PbUEo7YOr7WVan0ukRlEoldu7ciRdffFFvbXh4uN7XHBwcpLb2NprDwcEBGRkZspevbXhOFBFRNdE+TKc9H5SxuaGAkqvhSme47tixo8GQAVT9zYObNGkita9fv2603lBNbds2bQ4ODtIeqJs3bxrd+1VcXIwbN24AABo0aAAnJ6eqHqJBt27dkr73Q4YMMRigsrOzkZGRofd17XO0Ll++LGs8pX3k5ubizz//lNVHbcIQRURUTYKCgtCiRQsAJef/5OTkoLCwUJo0snHjxujbt6/OZdPT06U9NcbOL4qPj0d6enoljry8Ll26SO0jR44YrC0sLDR4+47K3jbtw2SiglNYKBQKdO7cGUDJB39sbKzB+tjYWGkvS+fOnavtkJ0+pSeKA8a/twcPHjQYEp9//nmpHRUVJWs8ldFHbcIQRURUjUqnLsjPz8ePP/6IvXv3Spf+jx07tsx5Utq0z8cxtucnIiKicgZrQOn90gDg0KFD+O233/TWfvfddwanN6jsbdM+1GjqIThDXn75ZaltbM6rhQsX6lyuppj6vX348CHmzZtnsK+BAwdKJ8NHRUXJmmJCe+qOL7/8slKuOqxJFQ5Ru3fvxvDhw+Hn5wdbW1u4u7sjKCgIixcvrtQrJ3r16qXzJpL6Hqbu7r1+/TpmzJiBNm3awNHREQ0bNkRAQADeeecdnZchExFVxLhx46QTpDdu3GjSoTyg5LBSy5YtAZTcsHb79u3lajQaDT788EMcOHCgkkddnpWVlTQHkkajwciRI6V5o7SdOnXK6NxKlb1tzZo1k9rnz583Wm/MhAkTpMC4b98+fPHFFzrrvvjiC+zfvx8A4OHhUeZGyDXl6aeflkLl7t27pWk0tOXn5+PVV1/FL7/8YrAvOzs7fPLJJwBKfh5DhgwxGKRu376NCxculPlaly5dpKkN7ty5g7CwMNy9e1dvH3FxcWX2ptU2sk8sf/DgAcaOHYvdu3eX+fq9e/dw7949nD59GitXrsS2bdtkTchVHb799lt88MEHyM/PL/P1a9eu4dq1a1i9ejVmz56N2bNn19AIiai+8fPzw/PPP49jx47h+PHj0p6nZ599Fm3btjW47AcffCCdLzRixAiMHDkSwcHBcHZ2xvXr17F582ZcuXIFbdq0gY2NDc6dO1el2zJ9+nTs2LED586dw+XLl9G6dWud986zsLDACy+8gH379lXLtrVt2xYeHh5ITU3F999/Dzc3N3Tr1q3MVYSl83aZomHDhtiwYQNeeOEFaDQazJ49GwcOHMDIkSPRuHFjpKSkYMuWLVJAsbS0xIYNG2rFSeXW1tZ4++238eWXX6KoqAjBwcGYMGECunTpAnt7e1y+fBkbNmxAUlISQkJCcPXqVYPzb02ZMgWxsbHYvn07UlNTERQUhLCwMPTr1w+NGzfGw4cPcfPmTRw7dgzHjh3DkiVL0L59+zJ9fPfdd+jWrRv++OMPxMXFoXnz5hg5cmSZe+dduXIF0dHR+PXXX3HhwoXaO2u5nHvKPHr0SAwYMKDM/XM+/fRT8cMPP4ivv/5a9OjRQ3rN2dlZXL58Wc5qyggODpb63Llzp9FHbm6uwf42bdok9WdhYSHGjBkj/v3vf4sNGzaIN954Q9jY2EivL1y4UNaYee88qgm8d17t9+9//7vcneq/+uoro8sVFxeLSZMmGbzjfdu2bcXNmzfL/M3UpTLunSeEEPfu3ROdO3fWOx5bW1vxww8/lLlnnK5751XmtgkhxNq1aw32Ze73Qggh9u7dK5ydnQ326+zsLPbu3au3D2PfB20xMTFSbUREhMFaQwoLC8t8Zut6BAcHi/T0dJN+5o8ePRJTp06V7u1n6LF8+XKdfaSnp4vQ0FCjywMQFy9eNHubq+veebJC1KpVq6SNCwwMFCkpKeVqpk2bJtX07NlTzmrKMOWXxlRpaWnCwcFBClBRUVHlak6fPi3s7OwEAGFpaSl+//13s9fDEEU1gSGq9svOzpb+vpT+jdH1d1SfHTt2iNDQUOHq6iqsrKyEp6enCA4OFl9//bX0c6+uECVEyYfqv/71L9GjRw/h5OQkGjRoIJo3by7efPNNcenSJSGE6eGhMrat1KFDh8TLL78smjZtKmxtbSscooQQIjMzU8yfP1/06NFDuLm5CUtLS+Hm5iaCgoLE3//+d5GZmWlw+ZoIUUIIodFoxL///W/x/PPPC0dHR2FtbS2aNGkiBgwYIDZs2CDdnNjUn7kQQly9elXMmDFDdOjQQbi4uAilUilUKpVo06aNmDRpkoiKihJFRUUG+zh8+LCYNGmSaNmypVCpVMLS0lK4urqKrl27imnTpokzZ87I2t5aG6IePXokGjduLP1gdd2hubTu2WefleoOHjxo7qrKqMwQ9dFHH0l9vffee3rrtO9WPXr0aLPXwxBFNYEhioiedNUVosw+sfz48ePSSWDBwcHl7tBcSqlU4v3335eeR0ZGmruqKlN6Q0cA+PDDD/XWhYeHS1c27N69u9y5U0RERPTkMjtEaV8ZERYWZrC2dDbex5erSZcvX8bt27cBAK1atSpzFcfjVCoVevbsCaDkMtljx45VyxiJiIio9jM7RP36669Su3QCMn08PT3h4+MDAEhNTcW9e/fMXZ1OgwYNQpMmTWBtbQ1nZ2e0bt0a4eHhiImJMbqsOeN/vEZ7WSIiInqymR2irl69KrUN7cXRVaO9bEXs27cPycnJKCoqQlZWFi5fvoy1a9eiT58+CAkJMTjnRG0YPxEREdV9Zs8TpT3rbOnMpYa4urrqXFYOZ2dn9OvXD506dUKTJk2gVCrx119/4fDhwzhw4ACEEDhy5Ai6d++OuLg4nfNKVOX4CwsLUVhYKD2vzMlGiYiIqHYxO0Rp33nZ1tbWaH2DBg2kdk5OjrmrkyxYsAAdO3aEtbV1udemTp2Ks2fP4uWXX8aff/6J27dvY9KkSdLMsdqqcvwLFizA3LlzjfZJREREdV+duXde9+7ddQaoUp06dUJ0dDRsbGwAlJzIHh8fX13DAwDMmjULarVaeiQlJVXr+omIiKj6mB2itG/sWFBQYLRee1qAqp4Cv1WrVmVubrh3795yNVU5fhsbGzg4OJR5EBERUf1kdohycnKS2unp6Ubr79+/r3PZqtK7d2+pfeXKlXKv1/bxExERUd1gdogKCAiQ2rdu3TJar12jvWxVadSokdTWdSJ4bR8/ERER1Q1mhyjtu4wbO+coNTVVOi/I3d29TMCpKtp7l3TtOTJn/I/XtGnTpmKDIyIionrD7BA1YMAAqW1sFnLtq+OMzW5eWbQn3NS15ygwMBBNmzYFUHK4LzExUW9fDx48wIkTJwAAdnZ2CA4OrtzBElUhIURND4GIqEZU198/s0NUcHCwNP/S0aNHcf78eZ11Go0GK1askJ6PGjVK5hBNd+3aNWzatEl6PmjQIJ11I0eOlNpfffWV3v6+/fZb5ObmAgBeeukl2NnZVdJIiaqOhUXJr3VxcXENj4SIqGZoNBoA//t7WFXM7l2pVGL27NnS8/HjxyMtLa1c3cyZM5GQkAAA6NGjB0JDQ3X2t379eigUCigUCvTq1UtnzYoVK3Dq1CmD47pw4QJCQ0OlK+769++Prl276qydPn26dKXdP//5T+zevbtczZkzZ/DZZ58BACwtLREREWFw/US1hZWVFZRKpfQPABHRkyYvLw9KpRJWVlZVuh6zJ9sEgPDwcOzcuROHDh3CpUuX0K5dO4SHhyMwMBAZGRmIjIzEyZMnAZScl7R69eoKDfLIkSOYMmUK/P390bdvX7Rp0waurq5QKpVITk7G4cOHsX//fuk/b19fX6xbt05vf+7u7li5ciUmTJiA4uJiDB06FKNGjUK/fv2gVCoRGxuLDRs2SIFs7ty5ePrppyu0DUTVRaFQQKVSITs7G40aNYJCoajpIRERVRshBLKzs6FSqar875+sEGVpaYkdO3ZgzJgx2Lt3L1JSUvDFF1+Uq/P29sbWrVvRunXrCg8UAG7cuIEbN24YrAkNDcV3330HLy8vg3WvvfYa8vLyMHXqVBQUFOCHH37ADz/8UKZGqVTik08+wccff1zhsRNVJ0dHR2RlZSE5ORleXl4MUkT0RBBCSPfWdXR0rPL1yQpRQMnEk3v27EFUVBQ2btyI+Ph4pKWlQaVSwd/fH8OGDcPkyZMrZSOWLl2KF198EWfOnMHFixeRlpaG9PR0FBYWwtHREX5+fujevTvGjh2r9xCeLm+99Rb69u2LVatWITo6GklJSSguLoaXlxdCQkLwxhtvoH379hUeP1F1s7Ozg7e3N+7cuYP8/Hw4ODjAzs4OSqWSgYqI6hUhBDQaDfLy8pCdnY2ioiJ4e3tXy3nMCsFLeKpMdnY2HB0doVarOXs51Yi8vDyo1Wrk5ORIJ1oSEdVHSqUSKpUKjo6OFQ5Qpn5+y94TRUS1n52dHezs7ODp6YmioiJesUdE9ZKFhQWsrKyqfU87QxTRE0ChUBi8gTcREZmvaidQICIiIqqnGKKIiIiIZGCIIiIiIpKBIYqIiIhIBoYoIiIiIhkYooiIiIhkYIgiIiIikoEhioiIiEgGhigiIiIiGRiiiIiIiGRgiCIiIiKSgSGKiIiISAaGKCIiIiIZGKKIiIiIZGCIIiIiIpKBIYqIiIhIBoYoIiIiIhkYooiIiIhkYIgiIiIikoEhioiIiEgGhigiIiIiGRiiiIiIiGRgiCIiIiKSgSGKiIiISAaGKCIiIiIZGKKIiIiIZGCIIiIiIpKBIYqIiIhIBoYoIiIiIhkYooiIiIhkYIgiIiIikoEhioiIiEgGhigiIiIiGRiiiIiIiGRgiCIiIiKSgSGKiIiISAaGKCIiIiIZGKKIiIiIZGCIIiIiIpKBIYqIiIhIBoYoIiIiIhkqHKJ2796N4cOHw8/PD7a2tnB3d0dQUBAWL16M7OzsyhijURMmTIBCoZAec+bMMWm5Bw8eYPny5ejXrx88PT1hY2ODRo0aoUuXLpg3bx5SU1OrduBERERUZymEEELOgg8ePMDYsWOxe/duvTU+Pj7Ytm0bunXrJnuAxhw4cABhYWFlvhYREWE0SMXExGD06NEGg5KLiwvWrFmDYcOGyRpbdnY2HB0doVar4eDgIKsPIiIiql6mfn5byulco9Fg+PDhiI6OBgB4eHggPDwcgYGByMjIQGRkJGJjY5GUlISwsDDExsaiVatW8rbEgOzsbEyePBkAYG9vj9zcXJOWO3HiBAYMGICHDx8CANq3b4/Ro0fD19cXarUahw4dwo4dO5CRkYGRI0di9+7dGDhwYKWPn4iIiOowIcOqVasEAAFABAYGipSUlHI106ZNk2p69uwpZzVGvfHGGwKA8PHxEVOnTpXWFxERoXeZwsJC4efnJ9V+8MEHori4uFzdoUOHhI2NjQAgPDw8RHZ2ttnjU6vVAoBQq9VmL0tEREQ1w9TPb7PPidJoNJg7d670fNOmTfDw8ChXt2jRIjz77LMASvb8/Pzzz+auyqAjR45gzZo1AIBvvvkGKpXKpOV27dqFxMREACV7oJYuXQqFQlGurm/fvvjkk08AAKmpqVi2bFmljJuIiIjqB7ND1PHjx3H37l0AQHBwMDp06KCzTqlU4v3335eeR0ZGyhxieXl5eQgPD4cQAiNHjsSgQYNMXvbw4cNSe/z48bCw0P8tmDhxotTevHmzvMESERFRvWR2iDpw4IDUfvyE7sdpn0ekvVxFzZo1Czdv3oSLiwuWL19u1rJ37tyR2gEBAQZrvb29YWdnBwC4evUq/vjjD/MHS0RERPWS2SHq119/ldqdO3c2WOvp6QkfHx8AJYfE7t27Z+7qyjl16hS+/vprAMCSJUt0Hko0RMi7GBEA8Msvv8heloiIiOoXs0PU1atXpXazZs2M1mvXaC8rR0FBASZNmoTi4mKEhISUOdxmKk9PT6l97do1g7V//fUX8vLypOe///672esjIiKi+snsEJWVlSW13dzcjNa7urrqXFaO2bNn4+rVq2jQoAFWr14tq4+ePXtK7Y0bNxrcM7V+/foyz42Nv7CwENnZ2WUeREREVD+ZHaIePHggtW1tbY3WN2jQQGrn5OSYuzpJfHw8vvrqKwDA3Llz4e/vL6ufV155BS4uLgCA8+fP46OPPtIZpGJiYjBv3rwyXzMWihYsWABHR0fpUXook4iIiOqfOnHvvIcPH2LSpEnQaDTo0KEDpk6dKrsvlUqFFStWSM+XLFmCzp07Y+nSpfjxxx+xdu1ajBo1Cv369UNBQUGZsGboSj6g5IR3tVotPZKSkmSPk4iIiGo3s2csb9iwITIzMwGUnKPUsGFDg/X5+flS29S5nB43b948/Pbbb1AqlVizZg2USqWsfkqNHTsW+fn5ePfdd1FYWIhz587h3LlzZWosLCwwd+5cZGZmSnNEOTs7G+zXxsYGNjY2FRobERER1Q1m74lycnKS2unp6Ubr79+/r3NZU128eBELFy4EAEydOlXvvFTm+r//+z9cv34ds2bNQseOHeHk5AQrKyv4+Phg7NixOHXqFGbPnl1m/NonpRMREdGTzew9UQEBAbh16xYA4NatW/Dz8zNYX1pbuqy51q9fj6KiIlhYWMDKyqrceUqljh8/XqZdWhcQEIDhw4frXMbb2xvz58/H/Pnz9a7/0qVLUrtLly5mj5+IiIjqJ7NDVNu2baUbD8fHx6N37956a1NTU6Xzgtzd3dGoUSOzB1h60ndxcbHBsKMtJiYGMTExAIDBgwfrDVHG3L9/X5oXq2HDhmjfvr2sfoiIiKj+Mftw3oABA6S2sVnI9+/fL7WNzW5eG23atAlFRUUAgHHjxvF8JyIiIpKYHaKCg4Olc4OOHj2K8+fP66zTaDRlroIbNWqUrAEuW7YMQgijj4iICGmZiIgI6eu7du2Std7k5GR8/vnnAABra2tMmTJFVj9ERERUP5kdopRKJWbPni09Hz9+PNLS0srVzZw5EwkJCQCAHj16IDQ0VGd/69evh0KhgEKhQK9evcwdjiy3bt3C7du39b5+6dIl9OnTR7oKce7cubLO5yIiIqL6y+xzogAgPDwcO3fuxKFDh3Dp0iW0a9cO4eHhCAwMREZGBiIjI3Hy5EkAJVfkyZ1dvKqcO3cOI0eORI8ePdCrVy+0aNECtra2SElJQUxMDPbs2YNHjx4BAF5//XV89NFHNTxiIiIiqm1khShLS0vs2LEDY8aMwd69e5GSkoIvvviiXJ23tze2bt2K1q1bV3igla24uBgnTpzAiRMndL5uZ2eHzz77DH/729+gUCiqeXRERERU28kKUUDJxJl79uxBVFQUNm7ciPj4eKSlpUGlUsHf3x/Dhg3D5MmT4ejoWJnjrRS9e/fG6tWrERMTg19++QWpqanIzs6Gm5sbnnrqKbzwwgsYP348mjRpUtNDJSIiolpKIQzdgZcqJDs7G46OjlCr1XBwcKjp4RAREZEJTP38rhP3ziMiIiKqbRiiiIiIiGRgiCIiIiKSgSGKiIiISAaGKCIiIiIZGKKIiIiIZGCIIiIiIpKBIYqIiIhIBoYoIiIiIhkYooiIiIhkYIgiIiIikoEhioiIiEgGhigiIiIiGRiiiIiIiGRgiCIiIiKSgSGKiIiISAaGKCIiIiIZGKKIiIiIZGCIIiIiIpKBIYqIiIhIBoYoIiIiIhkYooiIiIhkYIgiIiIikoEhioiIiEgGhigiIiIiGRiiiIiIiGRgiCIiIiKSgSGKiIiISAaGKCIiIiIZGKKIiIiIZGCIIiIiIpKBIYqIiIhIBoYoIiIiIhkYooiIiIhkYIgiIiIikoEhioiIiEgGhigiIiIiGRiiiIiIiGRgiCIiIiKSgSGKiIiISAaGKCIiIiIZGKKIiIiIZKhwiNq9ezeGDx8OPz8/2Nrawt3dHUFBQVi8eDGys7MrY4xGTZgwAQqFQnrMmTPHpOUKCgqwYcMGDB48GL6+vrCzs4O1tTXc3d3x/PPPY+7cuUhKSqrawRMREVGdpBBCCDkLPnjwAGPHjsXu3bv11vj4+GDbtm3o1q2b7AEac+DAAYSFhZX5WkREhNEglZCQgBEjRuCPP/4wWGdra4tFixbh/fffN3ts2dnZcHR0hFqthoODg9nLExERUfUz9fPbUk7nGo0Gw4cPR3R0NADAw8MD4eHhCAwMREZGBiIjIxEbG4ukpCSEhYUhNjYWrVq1krclBmRnZ2Py5MkAAHt7e+Tm5pq0XFJSEvr06YPMzEwAgLu7OyZMmICWLVvCxsYGiYmJiIyMxOXLl1FQUIApU6bAzs4O//d//1fp20BERER1lJBh1apVAoAAIAIDA0VKSkq5mmnTpkk1PXv2lLMao9544w0BQPj4+IipU6dK64uIiDC43Ouvvy7V9u/fX+Tm5parKS4uFh9//LFU16hRI1FUVGTW+NRqtQAg1Gq1WcsRERFRzTH189vsc6I0Gg3mzp0rPd+0aRM8PDzK1S1atAjPPvssAODEiRP4+eefzV2VQUeOHMGaNWsAAN988w1UKpXJy5buQQOAf/zjH7CzsytXo1Ao8Pnnn0vbdu/ePfz+++8VHDURERHVF2aHqOPHj+Pu3bsAgODgYHTo0EFnnVKpLHMeUWRkpMwhlpeXl4fw8HAIITBy5EgMGjTIrOXT0tKkdosWLfTWKZVKPPXUU9LzBw8emD9YIiIiqpfMDlEHDhyQ2o+f0P24gQMH6lyuombNmoWbN2/CxcUFy5cvN3t5d3d3qX3t2jW9dRqNBjdu3AAAWFpaIiAgwPzBEhERUb1kdoj69ddfpXbnzp0N1np6esLHxwcAkJqainv37pm7unJOnTqFr7/+GgCwZMkSnYcSjRkyZIjU/vDDD5GXl1euRgiBzz77TNprNWnSJDg7O8sbNBEREdU7Zl+dd/XqVandrFkzo/XNmjWT5lq6evUqGjVqZO4qJQUFBZg0aRKKi4sREhKCiRMnyupnzpw5+Pnnn/HHH3/g0KFDaNasGSZOnIiWLVvC2toat2/fRmRkJC5dugQAGD9+vKw9XlVCCKCofOgjIiJ6IlnZAQpFjaza7BCVlZUltd3c3IzWu7q66lxWjtmzZ+Pq1ato0KABVq9eLbsfNzc3nDlzBm+//Ta2b9+OtLQ0LFq0qFxd37598fHHH6N3794m9VtYWIjCwkLpeZVMNlqUB8z3qvx+iYiI6qKPkwFr+xpZtdkhSvvkaltbW6P1DRo0kNo5OTnmrk4SHx+Pr776CgAwd+5c+Pv7y+4LAJydnbFo0SI0atQIK1eu1Flz5MgRKBQKODs7S1caGrJgwYIyVy4SERFR/SVrss3q9vDhQ0yaNAkajQYdOnTA1KlTK9znkiVLMHPmTGg0GowbNw5vvfUW2rZtCysrK9y8eRPbtm3DwoULcejQIfTs2RM//vgjBgwYYLDPWbNmlRlbdna2dE5YpbGyK0ndREREVPK5WEPMDlENGzaUZvouKChAw4YNDdbn5+dLbXPmctI2b948/Pbbb1AqlVizZg2USqWsfkrNnj0bX3zxBQBg8eLFmD59epnXW7VqhYiICPTt2xd9+vTBgwcPMGrUKPzxxx8Gz+mysbGBjY1NhcZmlEJRY7stiYiI6H/MvjrPyclJaqenpxutv3//vs5lTXXx4kUsXLgQADB16lS981KZKjk5WeovICAA06ZN01vbo0cPjB8/HgCgVquxbt26Cq2biIiI6g+z90QFBATg1q1bAIBbt27Bz8/PYH1pbemy5lq/fj2KiopgYWEBKysrzJs3T2fd8ePHy7RL6wICAjB8+HDptZ9//hlFRUUASk4cVxg5o79///5Yu3YtAODMmTNmj5+IiIjqJ7NDVNu2baXbpsTHxxu8ci01NVWa3sDd3V3W9AZCCABAcXEx5s+fb9IyMTExiImJAQAMHjy4TIhKTv7f+USOjo5G+9Lee8YZy4mIiKiU2YfztE+uNjYL+f79+6W2sdnNq4v2eVmlAc+Q27dvS23t6RqIiIjoyWZ2iAoODoanpycA4OjRozh//rzOOo1GgxUrVkjPR40aJWuAy5YtgxDC6CMiIkJaJiIiQvr6rl27yvTXtm1bqb13716jczlt3rxZanfp0kXWNhAREVH9Y3aIUiqVmD17tvR8/PjxZW7oW2rmzJlISEgAUHKCdmhoqM7+1q9fD4VCAYVCgV69epk7HLP16NEDTZs2BQBkZmZi9OjRyM3NLVcnhMAnn3yCo0ePAiiZ72rEiBFVPj4iIiKqG2TNExUeHo6dO3fi0KFDuHTpEtq1a4fw8HAEBgYiIyMDkZGROHnyJICSc4oqMrt4ZbOyssLKlSsxdOhQFBcXY//+/WjZsiXGjRtXZp6orVu3SiEQAObPnw8vL84UTkRERCVkhShLS0vs2LEDY8aMwd69e5GSkiLNu6TN29sbW7duRevWrSs80Mr00ksvYcuWLZg8eTIyMzORnJys87YvQMncTwsXLsQHH3xQvYMkIiKiWk32jOUqlQp79uxBVFQUNm7ciPj4eKSlpUGlUsHf3x/Dhg3D5MmTTboCriYMHz4cISEh2LRpE6Kjo/HLL78gIyMDGo0GTk5OaNWqFXr37o3XX3+98mcdJyIiojpPIUrnEKBKl52dDUdHR6jVajg4ONT0cIiIiMgEpn5+m31iORERERExRBERERHJwhBFREREJIPsE8vJuNLTzYxN6ElERES1R+nntrHTxhmiqlBOTg4A8Oo+IiKiOignJ8fgLAO8Oq8KFRcXIzk5GSqVCgqFotL6zc7Oho+PD5KSknjVH9U7fH9Tfcb3d90ghEBOTg68vLxgYaH/zCfuiapCFhYW8Pb2rrL+HRwc+EtI9Rbf31Sf8f1d+5kyzyVPLCciIiKSgSGKiIiISAaGqDrIxsYGERERsLGxqemhEFU6vr+pPuP7u37hieVEREREMnBPFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0NUHbF7924MHz4cfn5+sLW1hbu7O4KCgrB48WLem49qnZycHOzYsQPvvvsugoKC0KhRI1hZWcHBwQFPP/00xo8fj+joaKP3pdJ2/fp1zJgxA23atIGjoyMaNmyIgIAAvPPOO0hISKi6jSEyw4QJE6BQKKTHnDlzTFqO7+86SlCtlpOTI1566SUBQO/Dx8dHnD59uqaHSiSEEGLp0qXC1tbW4Hu29NGzZ09x+/Zto32uXr1aNGjQQG8/SqVSzJ07txq2jki//fv3l3tvRkREGF2O7++6i7d9qcU0Gg2GDx+O6OhoAICHhwfCw8MRGBiIjIwMREZGIjY2FklJSQgLC0NsbCxatWpVw6OmJ921a9dQUFAAAGjSpAn69u2Ljh07wt3dHQUFBYiLi8P333+PBw8e4MSJE+jVqxfi4uLg7u6us7/vv/8ekydPBlByK6VRo0YhJCQElpaWiI2NxYYNG1BYWCjNvfO3v/2t2raVqFR2drb0PrW3t0dubq5Jy/H9XcfVdIoj/VatWiX9JxIYGChSUlLK1UybNq3Mf/VENe3NN98U/fv3Fz///LPQaDQ6axITE0VAQID03p04caLOurS0NOHg4CAACAsLCxEVFVWu5vTp08LOzk4AEJaWluL333+v1O0hMsUbb7whHRmYOnWqSXui+P6u+xiiaqlHjx6Jxo0bS7+I586d01v37LPPSnUHDx6s5pESlXX//n2T6hISEqT3rZ2dncjNzS1X89FHH0k17733nt6+li5dKtWNHj1a9tiJ5Dh8+LBQKBQCgNizZ4+IiIgwKUTx/V338cTyWur48eO4e/cuACA4OBgdOnTQWadUKvH+++9LzyMjI6tlfET6uLi4mFTXrl07BAQEAADy8vJw/fr1cjVbt26V2h9++KHevsLDw2Fvbw+g5CKM/Px8c4ZMJFteXh7Cw8MhhMDIkSMxaNAgk5fl+7vuY4iqpQ4cOCC1w8LCDNYOHDhQ53JEtZ2Dg4PUfvyD4fLly7h9+zYAoFWrVmjWrJneflQqFXr27AkAyM3NxbFjx6pgtETlzZo1Czdv3oSLiwuWL19u8nJ8f9cPDFG11K+//iq1O3fubLDW09MTPj4+AIDU1FTcu3evSsdGVBkePnyIa9euSc99fX3LvG7O78DjNdrLElWVU6dO4euvvwYALFmyBB4eHiYvy/d3/cAQVUtdvXpVahv6D0VXjfayRLXVDz/8ALVaDQDo0KEDPD09y7zO3wGqzQoKCjBp0iQUFxcjJCQEEydONGt5vr/rB4aoWiorK0tqu7m5Ga13dXXVuSxRbXTv3r0yl2p/+umn5Wr4O0C12ezZs3H16lU0aNAAq1evNnt5vr/rB4aoWurBgwdS29bW1mh9gwYNpHZOTk6VjImoMjx8+BAvv/wy0tLSAABDhgzB0KFDy9Xxd4Bqq/j4eHz11VcAgLlz58Lf39/sPvj+rh8Yooio2hQXF2PSpEk4ceIEAMDf3x/fffddDY+KyHQPHz7EpEmToNFo0KFDB0ydOrWmh0Q1iCGqlmrYsKHULp392RDtK5tUKlWVjImoIoQQePPNN7F582YAQNOmTfGf//wHzs7OOuv5O0C10bx58/Dbb79BqVRizZo1UCqVsvrh+7t+YIiqpZycnKR2enq60fr79+/rXJaoNhBC4O2338aaNWsAAN7e3jhy5Aj8/Pz0LsPfAaptLl68iIULFwIApk6dqnf+PlPw/V0/8N55tVRAQABu3boFALh165bBD5vSGu1liWoLIQTeeecdrFq1CkDJ/fRiYmKMnkei/T7Wfn/rw98Bqmrr169HUVERLCwsYGVlhXnz5umsO378eJl2aV1AQACGDx8utUvx/V13MUTVUm3btpVuPBwfH4/evXvrrU1NTUVSUhIAwN3dHY0aNaqWMRIZUxqg/vWvfwEAvLy8EBMTg+bNmxtdtm3btlI7Pj7eaL12TZs2bWSMlsgwIQSAknP75s+fb9IyMTExiImJAQAMHjxYClF8f9cPPJxXSw0YMEBqG5uFfP/+/VLb2OzmRNXl8QDVuHFjxMTEoEWLFiYtHxgYiKZNmwIArly5gsTERL21Dx48kE5Wt7OzQ3BwcMUGT1TF+P6uHxiiaqng4GBp8sGjR4/i/PnzOus0Gg1WrFghPR81alS1jI/ImHfffVcKUJ6enoiJiUHLli3N6mPkyJFSu/SScl2+/fZb5ObmAgBeeukl2NnZyRgxkWHLli2DEMLoIyIiQlomIiJC+vquXbvK9Mf3dz1Q3Xc8JtN988030p27W7duLVJTU8vVTJ8+Xarp0aNHDYySqLx3331Xel96enqK33//XVY/qampQqVSCQDCwsJCREVFlauJi4sTdnZ2AoCwtLQUV65cqejwiSokIiJCev9HREToreP7u+7jOVG1WHh4OHbu3IlDhw7h0qVLaNeuHcLDwxEYGIiMjAxERkbi5MmTAEqu1pAzay5RZfv000+l+4kpFApMmTIFV65cwZUrVwwu16FDB+nwRil3d3esXLkSEyZMQHFxMYYOHYpRo0ahX79+UCqViI2NxYYNG6RLxOfOnYunn366ajaMqJLx/V0P1HSKI8Oys7PFoEGDpP9qdD28vb1FbGxsTQ+VSAghRHBwsMH3q77HunXr9Pb5zTffCFtbW73LKpVKMXv27OrbSCIDTN0TVYrv77qLe6JqOZVKhT179iAqKgobN25EfHw80tLSoFKp4O/vj2HDhmHy5MlwdHSs6aESVZm33noLffv2xapVqxAdHY2kpCQUFxfDy8sLISEheOONN9C+ffuaHiaRLHx/110KIf7/NZtEREREZDJenUdEREQkA0MUERERkQwMUUREREQyMEQRERERycAQRURERCQDQxQRERGRDAxRRERERDIwRBERERHJwBBFREREJANDFBEREZEMDFFEREREMjBEEREREcnAEEVEREQkA0MUERERkQwMUUREREQy/D+V3wWjC8c45gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#history of training and validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training acc', 'validation acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#get access to filters/kernels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m weights \u001b[39m=\u001b[39m hiddenLayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mget_weights()[\u001b[39m0\u001b[39;49m][:, :, \u001b[39m0\u001b[39;49m, :] \u001b[39m#arr of a buncha nested lists, 0 bc only have 1 channel (28, 28, 1, 8)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#can get access to other params this way \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "#get access to filters/kernels\n",
    "weights = hiddenLayers[0].get_weights()[0][:, :, 0, :] #arr of a buncha nested lists, 0 bc only have 1 channel (28, 28, 1, 8)\n",
    "\n",
    "#can get access to other params this way \n",
    "\n",
    "for i in range(1, 8):\n",
    "    plt.subplot(2, 4, i)\n",
    "    plt.imshow(weights[:,:,i], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "#how get access to feature maps?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('EvoComp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fe32189b82d98f8d4c6c56307210a678aeb8b7128cbceacf7b4eb9894a56bcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
